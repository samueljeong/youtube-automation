# sermon_server.py
import os
from flask import Flask, render_template, request, jsonify
from openai import OpenAI

app = Flask(__name__)

def get_client():
    key = (os.getenv("OPENAI_API_KEY") or "").strip()
    if not key:
        raise RuntimeError("OPENAI_API_KEYê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    return OpenAI(api_key=key)

client = get_client()

@app.route("/")
def home():
    return render_template("sermon.html")

@app.route("/sermon")
def sermon():
    return render_template("sermon.html")

@app.route("/health")
def health():
    return jsonify({"ok": True})

# ===== í†µí•© ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸ =====
@app.route("/api/sermon/process", methods=["POST"])
def api_sermon_process():
    """
    ëª¨ë“  ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ì²˜ë¦¬í•˜ëŠ” í†µí•© ì—”ë“œí¬ì¸íŠ¸
    """
    try:
        data = request.get_json()
        if not data:
            return jsonify({"ok": False, "error": "No JSON data received"}), 400
        
        category = data.get("category", "")
        step_id = data.get("stepId", "")
        reference = data.get("reference", "")
        text = data.get("text", "")
        guide = data.get("guide", "")
        prompt_type = data.get("promptType", None)
        previous_results = data.get("previousResults", {})
        
        print(f"[PROCESS] category={category}, stepId={step_id}, promptType={prompt_type}")
        
        if not reference:
            return jsonify({"ok": False, "error": "ì„±ê²½ êµ¬ì ˆì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        # ë‹¨ê³„ë³„ ì²˜ë¦¬
        if step_id == "analysis":
            result = process_analysis(reference, text, guide, category, previous_results)
        elif step_id == "prompt":
            result = process_prompt(reference, text, guide, category, prompt_type, previous_results)
        else:
            # ì»¤ìŠ¤í…€ ë‹¨ê³„ (ì¼ë°˜ ì²˜ë¦¬)
            result = process_custom_step(step_id, reference, text, guide, category, previous_results)
        
        return jsonify({"ok": True, "result": result})
        
    except Exception as e:
        err_text = str(e)
        print(f"[PROCESS][ERROR] {err_text}")
        return jsonify({"ok": False, "error": err_text}), 200


def process_analysis(reference, text, guide, category, previous_results):
    """ë³¸ë¬¸ ë¶„ì„ ì²˜ë¦¬"""
    content = f"[ì„±ê²½ êµ¬ì ˆ]\n{reference}\n\n[ì¹´í…Œê³ ë¦¬]\n{category}"
    
    if text:
        content += f"\n\n[ë³¸ë¬¸ ë‚´ìš©]\n{text}"
    
    if guide:
        content = f"[ì‚¬ìš©ì ì§€ì¹¨]\n{guide}\n\n{content}"
    
    content += "\n\nìœ„ ë³¸ë¬¸ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
    
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": "You help Korean pastors analyze Bible passages."
            },
            {
                "role": "user",
                "content": content
            }
        ],
        temperature=0.7,
    )
    
    return completion.choices[0].message.content.strip()


def process_prompt(reference, text, guide, category, prompt_type, previous_results):
    """ì„¤êµë¬¸ í”„ë¡¬í”„íŠ¸ ìƒì„± ì²˜ë¦¬"""
    
    # ë©”íƒ€-í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    content = f"""ë‹¹ì‹ ì€ ì„¤êµë¬¸ ì‘ì„± í”„ë¡¬í”„íŠ¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, **ë‹¤ë¥¸ GPT ëª¨ë¸(ì˜ˆ: ChatGPT Plus, Claude)ì—ê²Œ ì§ì ‘ ì…ë ¥í•  ìˆ˜ ìˆëŠ” ì™„ì„±ëœ ì„¤êµë¬¸ ì‘ì„± í”„ë¡¬í”„íŠ¸**ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

---

ğŸ“Œ **ì„¤êµ ì •ë³´**
- ì„±ê²½ êµ¬ì ˆ: {reference}
- ì¹´í…Œê³ ë¦¬: {category}
- ì„¤êµ ìœ í˜•: {prompt_type or 'ê¸°ë³¸'}
"""

    if text:
        content += f"\n- ë³¸ë¬¸ ë‚´ìš©:\n{text}\n"
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ë“¤ í¬í•¨
    if previous_results:
        content += "\nğŸ“Š **ì´ì „ ë‹¨ê³„ ê²°ê³¼**\n"
        for step_id, step_data in previous_results.items():
            content += f"\n[{step_data['name']}]\n{step_data['result']}\n"
        content += "\nâš ï¸ **ì¤‘ìš”**: ìœ„ì˜ ì´ì „ ë‹¨ê³„ ê²°ê³¼ë“¤ì„ í”„ë¡¬í”„íŠ¸ì— ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œ, GPTê°€ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¤êµë¬¸ì„ ì‘ì„±í•˜ë„ë¡ í•´ì£¼ì„¸ìš”.\n"
    
    if guide:
        content += f"""
ğŸ“˜ **ì„¤êµ ì œì‘ ë§¤ë‰´ì–¼ (í•„ìˆ˜ ì¤€ìˆ˜)**
{guide}

âš ï¸ **ì¤‘ìš”**: ìœ„ ë§¤ë‰´ì–¼ì˜ ëª¨ë“  ì§€ì¹¨ì„ í”„ë¡¬í”„íŠ¸ì— ëª…í™•íˆ í¬í•¨ì‹œì¼œì£¼ì„¸ìš”.
"""
    
    content += """

---

âœ… **ì¶œë ¥ í˜•ì‹**

ì•„ë˜ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ **ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸**ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:
```
[GPTì—ê²Œ ì…ë ¥í•  í”„ë¡¬í”„íŠ¸ ì‹œì‘]

ë‹¹ì‹ ì€ í•œêµ­ êµíšŒì˜ ì„¤êµë¬¸ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì„¤êµ ì •ë³´ì™€ ë§¤ë‰´ì–¼ì„ í†µí•©í•˜ì—¬ ëª…í™•í•œ ì§€ì‹œì‚¬í•­ ì‘ì„±]
[ì´ì „ ë‹¨ê³„ ê²°ê³¼ë“¤ì„ í¬í•¨]
[ê¸°ëŒ€í•˜ëŠ” ì„¤êµë¬¸ì˜ êµ¬ì¡°ì™€ í†¤ ëª…ì‹œ]
[êµ¬ì²´ì ì¸ ì‘ì„± ì§€ì¹¨]

[GPTì—ê²Œ ì…ë ¥í•  í”„ë¡¬í”„íŠ¸ ë]
```

**ì£¼ì˜ì‚¬í•­**:
1. í”„ë¡¬í”„íŠ¸ëŠ” ë³µì‚¬-ë¶™ì—¬ë„£ê¸°ë§Œ í•˜ë©´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤
2. ì„¤êµë¬¸ì„ ì§ì ‘ ì‘ì„±í•˜ì§€ ë§ê³ , "ì„¤êµë¬¸ì„ ì‘ì„±í•˜ë¼"ëŠ” ì§€ì‹œë¬¸ì„ ë§Œë“œì„¸ìš”
3. ë§¤ë‰´ì–¼ì˜ ëª¨ë“  ì„¸ë¶€ì‚¬í•­ì´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
4. ì´ì „ ë‹¨ê³„ì˜ ëª¨ë“  ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ì— í†µí•©í•˜ì„¸ìš”
"""

    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": "You are an expert at creating sermon writing prompts for other AI models. You create clear, detailed prompts that other GPTs can use to write excellent sermons. You NEVER write the sermon itself - you only create the prompt."
            },
            {
                "role": "user",
                "content": content
            }
        ],
        temperature=0.7,
    )
    
    return completion.choices[0].message.content.strip()


def process_custom_step(step_id, reference, text, guide, category, previous_results):
    """ì»¤ìŠ¤í…€ ë‹¨ê³„ ì²˜ë¦¬ (ì§ˆë¬¸ ìƒì„±, í† ë¡  ì£¼ì œ ë“±)"""
    
    # ë‹¨ê³„ ì´ë¦„ì„ ì¶”ë¡  (ì‹¤ì œë¡œëŠ” í”„ë¡ íŠ¸ì—ì„œ ë³´ë‚´ì£¼ëŠ” ê²Œ ì¢‹ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ)
    step_names = {
        "questions": "ì„±ê²½ê³µë¶€ ì§ˆë¬¸",
        "discussion": "í† ë¡  ì£¼ì œ",
        "application": "ì‹¤ì²œ ê³¼ì œ",
        "prayer": "ê¸°ë„ ì œëª©",
        "illustration": "ì˜ˆí™”",
        "outline": "ì„¤êµ ê°œìš”"
    }
    
    step_name = step_names.get(step_id, step_id)
    
    content = f"""[ì„±ê²½ êµ¬ì ˆ]\n{reference}\n\n[ì¹´í…Œê³ ë¦¬]\n{category}"""
    
    if text:
        content += f"\n\n[ë³¸ë¬¸ ë‚´ìš©]\n{text}"
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ë“¤ í¬í•¨
    if previous_results:
        content += "\n\n[ì´ì „ ë‹¨ê³„ ê²°ê³¼]\n"
        for prev_step_id, step_data in previous_results.items():
            content += f"\n## {step_data['name']}\n{step_data['result']}\n"
    
    if guide:
        content = f"[ì‚¬ìš©ì ì§€ì¹¨]\n{guide}\n\n{content}"
    
    content += f"\n\nìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {step_name}ì„(ë¥¼) ì‘ì„±í•´ì£¼ì„¸ìš”."
    
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": f"You are a Korean church ministry expert helping to create {step_name}."
            },
            {
                "role": "user",
                "content": content
            }
        ],
        temperature=0.7,
    )
    
    return completion.choices[0].message.content.strip()


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5057))
    app.run(host="0.0.0.0", port=port, debug=False)
