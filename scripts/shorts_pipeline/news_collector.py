"""
ì‡¼ì¸  íŒŒì´í”„ë¼ì¸ - ì—°ì˜ˆ ë‰´ìŠ¤ ìˆ˜ì§‘

Google News RSSì—ì„œ ì—°ì˜ˆ ë‰´ìŠ¤ ìˆ˜ì§‘
ì—°ì˜ˆì¸ ì´ë¦„ ì¶”ì¶œ ë° ì´ìŠˆ ë¶„ë¥˜
"""

import re
import hashlib
import feedparser
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
from urllib.parse import quote

from .config import (
    RSS_FEEDS,
    ENTERTAINMENT_RSS_FEEDS,
    ISSUE_TYPES,
    CELEBRITY_SILHOUETTES,
    ATHLETE_SILHOUETTES,
    KOREA_PRIDE_SILHOUETTES,
    CONTENT_CATEGORIES,
)

# ë°”ì´ëŸ´ ì ìˆ˜í™” ë° ëŒ“ê¸€ ë¶„ì„
from .news_scorer import (
    analyze_news_viral_potential,
    rank_news_by_viral_potential,
)


def google_news_rss_url(query: str) -> str:
    """Google News RSS URL ìƒì„±"""
    encoded = quote(query)
    return f"https://news.google.com/rss/search?q={encoded}&hl=ko&gl=KR&ceid=KR:ko"


def fetch_rss_feed(url: str, max_items: int = 20) -> List[Dict[str, Any]]:
    """
    RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ í•­ëª© ê°€ì ¸ì˜¤ê¸°

    Returns:
        [{"title": "...", "link": "...", "published": "...", "summary": "..."}, ...]
    """
    try:
        feed = feedparser.parse(url)
        items = []

        for entry in feed.entries[:max_items]:
            items.append({
                "title": entry.get("title", ""),
                "link": entry.get("link", ""),
                "published": entry.get("published", ""),
                "summary": entry.get("summary", ""),
            })

        return items

    except Exception as e:
        print(f"[SHORTS] RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []


def extract_celebrity_name(text: str) -> Optional[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ì—°ì˜ˆì¸ ì´ë¦„ ì¶”ì¶œ

    íŒ¨í„´:
    - "ë°•ë‚˜ë˜ê°€", "ë°•ë‚˜ë˜ì˜", "ë°•ë‚˜ë˜,"
    - ì•Œë ¤ì§„ ì—°ì˜ˆì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë§¤ì¹­

    Returns:
        ì—°ì˜ˆì¸ ì´ë¦„ ë˜ëŠ” None
    """
    # 0) ê°€ì¡± í˜¸ì¹­ ë“± ì¼ë°˜ ëª…ì‚¬ í•„í„° (ë¨¼ì € ì •ì˜)
    exclude_names = {
        # ê°€ì¡±/ê´€ê³„ í˜¸ì¹­
        "ì—„ë§ˆ", "ì•„ë¹ ", "ì•„ë²„ì§€", "ì–´ë¨¸ë‹ˆ", "ì•„ë“¤", "ë”¸", "ë‚¨í¸", "ì•„ë‚´",
        "ì–¸ë‹ˆ", "ì˜¤ë¹ ", "ëˆ„ë‚˜", "í˜•", "ë™ìƒ", "í• ë¨¸ë‹ˆ", "í• ì•„ë²„ì§€",
        "ì™¸ê³„ì¸", "ìŠˆí¼ë§˜", "ì›Œí‚¹ë§˜", "ìœ¡ì•„",
        # ì§í•¨/ì—­í• 
        "ëŒ€í†µë ¹", "ë„¤í‹°ì¦Œ", "ì‹œì²­ì", "íŒ¬ë“¤", "ê´€ê³„ì", "ë§¤ë‹ˆì €", "ê¸°ì",
        # ë°©ì†¡ ìš©ì–´
        "ë“œë¼ë§ˆ", "ì˜ˆëŠ¥", "ì˜í™”", "ë°©ì†¡", "í”„ë¡œê·¸ë¨", "ì½˜í…ì¸ ", "ìœ íŠœë¸Œ",
        "ì±„ë„", "ì‹œì¦Œ", "ì—í”¼ì†Œë“œ", "ë¬´ëŒ€", "ì•¨ë²”", "ë®¤ì§ë¹„ë””ì˜¤",
        # ì¼ë°˜ ëª…ì‚¬
        "ì—°ì˜ˆì¸", "ì•„ì´ëŒ", "ë°°ìš°", "ê°€ìˆ˜", "ì½”ë¯¸ë””ì–¸", "ê°œê·¸ë§¨",
        "ì„ ìˆ˜", "ê°ë…", "ì½”ì¹˜", "ì‹¬íŒ",
        # ì¶”ìƒ ëª…ì‚¬
        "ë…¼ë€", "ì‚¬ê±´", "ì´ìŠˆ", "ë¬¸ì œ", "ìƒí™©", "ê²°ê³¼", "ì˜í–¥",
        "ë„íŒŒë¯¼", "ë‚˜ë½", "íŠ¸ë Œë“œ", "í˜„ìƒ", "ì—°ë§ê²°ì‚°", "ë² ìŠ¤íŠ¸", "ìµœê³ ", "ìµœì•…", "ìˆœìœ„",
    }

    # 1) ì•Œë ¤ì§„ ì—°ì˜ˆì¸ ëª©ë¡ì—ì„œ ì°¾ê¸° (ìµœìš°ì„ )
    for celeb in CELEBRITY_SILHOUETTES.keys():
        if celeb in text and celeb not in ["default_male", "default_female"]:
            return celeb

    # 1-1) ìš´ë™ì„ ìˆ˜ ëª©ë¡ì—ì„œë„ ì°¾ê¸°
    for athlete in ATHLETE_SILHOUETTES.keys():
        if athlete in text and athlete not in ["default_athlete"]:
            return athlete

    # 2) â˜… ë‰´ìŠ¤ ì œëª© íŒ¨í„´: "ì´ì‹œì˜," "ë°•ë‚˜ë˜ê°€" ê°™ì€ ì‹¤ëª… íŒ¨í„´ ìš°ì„ 
    #    - ì‰¼í‘œ/ë§ˆì¹¨í‘œ ì•ì˜ 3ê¸€ì ì´ë¦„ (ë‰´ìŠ¤ ì œëª©ì—ì„œ í”í•œ íŒ¨í„´)
    real_name_patterns = [
        r'\.\.([ê°€-í£]{2,4}),',          # "..ì´ì‹œì˜," íŒ¨í„´
        r'([ê°€-í£]{2,4}),\s*[éŸ“í•œ]',      # "ì´ì‹œì˜, éŸ“" íŒ¨í„´
        r'([ê°€-í£]{2,4})\s*[\'\"]\s*[ê°€-í£]',  # "ì´ì‹œì˜ 'ì—„ë§ˆ'" íŒ¨í„´
        r'^([ê°€-í£]{2,4})(?:ê°€|ì´|ëŠ”|,)',  # ë¬¸ì¥ ì‹œì‘ "ë°•ë‚˜ë˜ê°€"
    ]

    for pattern in real_name_patterns:
        match = re.search(pattern, text)
        if match:
            name = match.group(1)
            if name not in exclude_names and len(name) >= 2:
                return name

    # 3) ì¼ë°˜ í•œê¸€ ì´ë¦„ íŒ¨í„´ (2-4ê¸€ì + ì¡°ì‚¬) - fallback
    patterns = [
        r'([ê°€-í£]{2,4})(?:ê°€|ì´|ëŠ”|ì˜|ë¥¼|ì—ê²Œ|ì¸¡|ì”¨)',
        r'\'([ê°€-í£]{2,4})\'',
        r'\"([ê°€-í£]{2,4})\"',
    ]

    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            name = match.group(1)
            # ì¼ë°˜ ëª…ì‚¬/ë°©ì†¡ ìš©ì–´ í•„í„°ë§
            exclude = [
                # ì§í•¨/ì—­í• 
                "ëŒ€í†µë ¹", "ë„¤í‹°ì¦Œ", "ì‹œì²­ì", "íŒ¬ë“¤", "ê´€ê³„ì", "ë§¤ë‹ˆì €", "ê¸°ì",
                # ë°©ì†¡ ìš©ì–´
                "ë“œë¼ë§ˆ", "ì˜ˆëŠ¥", "ì˜í™”", "ë°©ì†¡", "í”„ë¡œê·¸ë¨", "ì½˜í…ì¸ ", "ìœ íŠœë¸Œ",
                "ì±„ë„", "ì‹œì¦Œ", "ì—í”¼ì†Œë“œ", "ë¬´ëŒ€", "ì•¨ë²”", "ë®¤ì§ë¹„ë””ì˜¤",
                # ì¼ë°˜ ëª…ì‚¬
                "ì—°ì˜ˆì¸", "ì•„ì´ëŒ", "ë°°ìš°", "ê°€ìˆ˜", "ì½”ë¯¸ë””ì–¸", "ê°œê·¸ë§¨",
                "ì„ ìˆ˜", "ê°ë…", "ì½”ì¹˜", "ì‹¬íŒ",
                # ì¶”ìƒ ëª…ì‚¬
                "ë…¼ë€", "ì‚¬ê±´", "ì´ìŠˆ", "ë¬¸ì œ", "ìƒí™©", "ê²°ê³¼", "ì˜í–¥",
                "ë„íŒŒë¯¼", "ë‚˜ë½", "íŠ¸ë Œë“œ", "í˜„ìƒ",
                # â˜… ê°€ì¡±/ê´€ê³„ í˜¸ì¹­ (ì´ì‹œì˜ 'ì—„ë§ˆ' ì˜¤ì¸ì‹ ë°©ì§€)
                "ì—„ë§ˆ", "ì•„ë¹ ", "ì•„ë²„ì§€", "ì–´ë¨¸ë‹ˆ", "ì•„ë“¤", "ë”¸", "ë‚¨í¸", "ì•„ë‚´",
                "ì–¸ë‹ˆ", "ì˜¤ë¹ ", "ëˆ„ë‚˜", "í˜•", "ë™ìƒ", "í• ë¨¸ë‹ˆ", "í• ì•„ë²„ì§€",
                "ì™¸ê³„ì¸", "ìŠˆí¼ë§˜", "ì›Œí‚¹ë§˜", "ìœ¡ì•„",
                # â˜… ì—°ë§ê²°ì‚°/ë­í‚¹ ìš©ì–´
                "ì—°ë§ê²°ì‚°", "ë² ìŠ¤íŠ¸", "ìµœê³ ", "ìµœì•…", "ìˆœìœ„",
            ]
            if name not in exclude:
                return name

    return None


def detect_issue_type(text: str) -> str:
    """
    ë‰´ìŠ¤ í…ìŠ¤íŠ¸ì—ì„œ ì´ìŠˆ ìœ í˜• ê°ì§€

    Returns:
        ë…¼ë€/ì—´ì• /ì»´ë°±/ì‚¬ê±´/ê·¼í™©
    """
    keywords = {
        "ë…¼ë€": ["ë…¼ë€", "ê°‘ì§ˆ", "í•™í­", "í­ë¡œ", "ë¹„íŒ", "ì‚¬ê³¼", "í•´ëª…", "ì˜í˜¹"],
        "ì—´ì• ": ["ì—´ì• ", "ê²°í˜¼", "ì´í˜¼", "íŒŒí˜¼", "ì—°ì¸", "ì»¤í”Œ", "êµì œ"],
        "ì»´ë°±": ["ì»´ë°±", "ì‹ ê³¡", "ì•¨ë²”", "ë°œë§¤", "í™œë™", "ë¬´ëŒ€", "ë°ë·”"],
        "ì‚¬ê±´": ["ì‚¬ê³ ", "ì†Œì†¡", "êµ¬ì†", "ì²´í¬", "ê¸°ì†Œ", "ì¬íŒ", "ì‚¬ë§"],
        "ê·¼í™©": ["ê·¼í™©", "ë³µê·€", "í™œë™", "ë°©ì†¡", "ì¶œì—°", "ì¸ìŠ¤íƒ€"],
    }

    for issue_type, words in keywords.items():
        for word in words:
            if word in text:
                return issue_type

    return "ê·¼í™©"  # ê¸°ë³¸ê°’


def get_silhouette_description(
    person: str,
    category: str = "ì—°ì˜ˆì¸",
    use_dynamic: bool = True
) -> str:
    """
    ì¸ë¬¼ì— ë§ëŠ” ì‹¤ë£¨ì—£ ì„¤ëª… ë°˜í™˜

    Args:
        person: ì¸ë¬¼ ì´ë¦„
        category: ì¹´í…Œê³ ë¦¬ (ì—°ì˜ˆì¸/ìš´ë™ì„ ìˆ˜/êµ­ë½•)
        use_dynamic: ë™ì  ìƒì„± ì‚¬ìš© ì—¬ë¶€ (Google ê²€ìƒ‰ + LLM)

    Returns:
        ì‹¤ë£¨ì—£ í”„ë¡¬í”„íŠ¸ ì„¤ëª… (ì˜ì–´)
    """
    # 1) ì¹´í…Œê³ ë¦¬ë³„ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ (ì •ì  ë¼ì´ë¸ŒëŸ¬ë¦¬ ìš°ì„ )
    if category == "ìš´ë™ì„ ìˆ˜" and person in ATHLETE_SILHOUETTES:
        return ATHLETE_SILHOUETTES[person]
    elif category == "êµ­ë½•":
        return KOREA_PRIDE_SILHOUETTES.get("default", KOREA_PRIDE_SILHOUETTES["default"])
    elif person in CELEBRITY_SILHOUETTES:
        return CELEBRITY_SILHOUETTES[person]
    elif person in ATHLETE_SILHOUETTES:
        return ATHLETE_SILHOUETTES[person]

    # 2) â˜… ë™ì  ì‹¤ë£¨ì—£ ìƒì„± (Google ê²€ìƒ‰ + LLM)
    if use_dynamic and person:
        try:
            from .silhouette_generator import generate_silhouette_dynamic

            print(f"[NewsCollector] ë™ì  ì‹¤ë£¨ì—£ ìƒì„± ì‹œë„: {person}")
            result = generate_silhouette_dynamic(person, category)
            if result:
                return result
        except ImportError as e:
            print(f"[NewsCollector] ë™ì  ìƒì„± ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
        except Exception as e:
            print(f"[NewsCollector] ë™ì  ìƒì„± ì‹¤íŒ¨ (fallback ì‚¬ìš©): {e}")

    # 3) Fallback: ì„±ë³„ ì¶”ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
    if person:
        # ì—¬ì„±ì— ë§ì€ ëê¸€ì
        female_endings = ["í¬", "ì˜", "ê²½", "ìˆ™", "ì •", "ì—°", "ì•„", "ì´", "ë‚˜", "ë¼", "ì§€", "ì€", "í˜„"]
        if person[-1] in female_endings:
            return CELEBRITY_SILHOUETTES.get("default_female", "female figure in casual standing pose")

    return CELEBRITY_SILHOUETTES.get("default_male", "male figure in casual standing pose")


def summarize_news(title: str, summary: str, max_length: int = 150) -> str:
    """
    ë‰´ìŠ¤ ìš”ì•½ ìƒì„± (3ì¤„ ì´ë‚´)
    """
    # HTML íƒœê·¸ ì œê±°
    clean_summary = re.sub(r'<[^>]+>', '', summary)
    clean_summary = re.sub(r'\s+', ' ', clean_summary).strip()

    # ì œëª© + ìš”ì•½
    full_text = f"{title}. {clean_summary}"

    if len(full_text) > max_length:
        full_text = full_text[:max_length] + "..."

    return full_text


def generate_hook_text(celebrity: str, issue_type: str, title: str) -> str:
    """
    í›… ë¬¸ì¥ ìƒì„± (ì²« 3ì´ˆ)
    """
    hooks = {
        "ë…¼ë€": [
            f"{celebrity}, ì´ë²ˆì—” ì§„ì§œ ëì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤",
            f"{celebrity}ì˜ ì¶©ê²©ì ì¸ ì§„ì‹¤ì´ ë°í˜€ì¡ŒìŠµë‹ˆë‹¤",
            f"{celebrity}, ê²°êµ­ ì´ë ‡ê²Œ ëìŠµë‹ˆë‹¤",
        ],
        "ì—´ì• ": [
            f"{celebrity}ì˜ ë¹„ë°€ ì—°ì¸ì´ ê³µê°œëìŠµë‹ˆë‹¤",
            f"{celebrity}, ê²°í˜¼ ë°œí‘œí–ˆìŠµë‹ˆë‹¤",
            f"{celebrity}ì˜ ìƒˆë¡œìš´ ì‹œì‘ì…ë‹ˆë‹¤",
        ],
        "ì»´ë°±": [
            f"{celebrity}ê°€ ëŒì•„ì˜µë‹ˆë‹¤",
            f"{celebrity}ì˜ ì—­ëŒ€ê¸‰ ì»´ë°±ì…ë‹ˆë‹¤",
            f"ë“œë””ì–´ {celebrity}ê°€ ì»´ë°±í•©ë‹ˆë‹¤",
        ],
        "ì‚¬ê±´": [
            f"{celebrity}ì—ê²Œ ë¬´ìŠ¨ ì¼ì´ ìƒê²¼ìŠµë‹ˆë‹¤",
            f"{celebrity}, ì¶©ê²©ì ì¸ ì†Œì‹ì…ë‹ˆë‹¤",
            f"{celebrity}ì˜ í˜„ì¬ ìƒí™©ì…ë‹ˆë‹¤",
        ],
        "ê·¼í™©": [
            f"{celebrity}ì˜ ìµœê·¼ ëª¨ìŠµì…ë‹ˆë‹¤",
            f"{celebrity}, ìš”ì¦˜ ì´ë ‡ê²Œ ì§€ëƒ…ë‹ˆë‹¤",
            f"ì˜¤ëœë§Œì— {celebrity} ì†Œì‹ì…ë‹ˆë‹¤",
        ],
    }

    import random
    return random.choice(hooks.get(issue_type, hooks["ê·¼í™©"]))


def compute_hash(text: str) -> str:
    """í…ìŠ¤íŠ¸ í•´ì‹œ ìƒì„± (ì¤‘ë³µ ì²´í¬ìš©)"""
    return hashlib.md5(text.encode()).hexdigest()[:12]


def collect_entertainment_news(
    max_per_feed: int = 10,
    total_limit: int = 20,
    categories: List[str] = None
) -> List[Dict[str, Any]]:
    """
    ë‰´ìŠ¤ ìˆ˜ì§‘ ë©”ì¸ í•¨ìˆ˜ (ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì§€ì›)

    Args:
        max_per_feed: í”¼ë“œë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ìˆ˜
        total_limit: ì „ì²´ ìµœëŒ€ ìˆ˜ì§‘ ìˆ˜
        categories: ìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ ëª©ë¡ (Noneì´ë©´ ì „ì²´)

    Returns:
        [
            {
                "run_id": "2024-12-24",
                "category": "ì—°ì˜ˆì¸",
                "person": "ë°•ë‚˜ë˜",
                "issue_type": "ë…¼ë€",
                "news_title": "...",
                "news_url": "...",
                "news_summary": "...",
                "silhouette_desc": "...",
                "hook_text": "...",
                "ìƒíƒœ": "ì¤€ë¹„",  # ì‚¬ìš©ìê°€ "ëŒ€ê¸°"ë¡œ ë³€ê²½í•´ì•¼ ì²˜ë¦¬ë¨
            },
            ...
        ]
    """
    all_items = []
    seen_hashes = set()
    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    # ìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ ê²°ì •
    if categories is None:
        categories = CONTENT_CATEGORIES  # ["ì—°ì˜ˆì¸", "ìš´ë™ì„ ìˆ˜", "êµ­ë½•"]

    for category in categories:
        if category not in RSS_FEEDS:
            print(f"[SHORTS] ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬: {category}")
            continue

        feeds = RSS_FEEDS[category]
        print(f"[SHORTS] === {category} ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì‹œì‘ ===")

        for feed_config in feeds:
            feed_name = feed_config["name"]
            feed_url = feed_config["url"]

            print(f"[SHORTS] RSS ìˆ˜ì§‘ ì¤‘: {feed_name}")
            items = fetch_rss_feed(feed_url, max_items=max_per_feed)

            for item in items:
                title = item["title"]
                link = item["link"]
                summary = item.get("summary", "")

                # ì¸ë¬¼ ì´ë¦„ ì¶”ì¶œ
                person = extract_celebrity_name(title + " " + summary)
                if not person:
                    continue  # ì¸ë¬¼ ì´ë¦„ ì—†ìœ¼ë©´ ìŠ¤í‚µ

                # ì¤‘ë³µ ì²´í¬
                item_hash = compute_hash(person + link)
                if item_hash in seen_hashes:
                    continue
                seen_hashes.add(item_hash)

                # ì´ìŠˆ ìœ í˜• ê°ì§€
                issue_type = detect_issue_type(title + " " + summary)

                # ì‹¤ë£¨ì—£ ì„¤ëª…
                silhouette_desc = get_silhouette_description(person, category)

                # í›… ë¬¸ì¥
                hook_text = generate_hook_text(person, issue_type, title)

                # ë‰´ìŠ¤ ìš”ì•½
                news_summary = summarize_news(title, summary)

                all_items.append({
                    "run_id": today,
                    "category": category,        # âœ… ì¹´í…Œê³ ë¦¬ ì¶”ê°€
                    "person": person,            # âœ… celebrity â†’ person
                    "issue_type": issue_type,
                    "news_title": title,
                    "news_url": link,
                    "news_summary": news_summary,
                    "silhouette_desc": silhouette_desc,
                    "hook_text": hook_text,
                    "ìƒíƒœ": "ì¤€ë¹„",  # ì‚¬ìš©ìê°€ "ëŒ€ê¸°"ë¡œ ë³€ê²½í•´ì•¼ ì²˜ë¦¬ë¨
                })

                if len(all_items) >= total_limit:
                    break

            if len(all_items) >= total_limit:
                break

        if len(all_items) >= total_limit:
            break

    print(f"[SHORTS] ì´ {len(all_items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
    return all_items


def search_celebrity_news(
    person: str,
    category: str = "ì—°ì˜ˆì¸",
    max_items: int = 5
) -> List[Dict[str, Any]]:
    """
    íŠ¹ì • ì¸ë¬¼ ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰

    Args:
        person: ì¸ë¬¼ ì´ë¦„
        category: ì¹´í…Œê³ ë¦¬ (ì—°ì˜ˆì¸/ìš´ë™ì„ ìˆ˜/êµ­ë½•)
        max_items: ìµœëŒ€ ë°˜í™˜ ìˆ˜

    Returns:
        ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ëª©ë¡
    """
    # ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ ì¿¼ë¦¬
    if category == "ìš´ë™ì„ ìˆ˜":
        query = f"{person} ì„ ìˆ˜"
    elif category == "êµ­ë½•":
        query = f"{person} í•œêµ­"
    else:
        query = f"{person} ì—°ì˜ˆ"

    url = google_news_rss_url(query)

    print(f"[SHORTS] '{person}' ({category}) ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
    items = fetch_rss_feed(url, max_items=max_items)

    results = []
    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    for item in items:
        title = item["title"]
        link = item["link"]
        summary = item.get("summary", "")

        issue_type = detect_issue_type(title + " " + summary)
        silhouette_desc = get_silhouette_description(person, category)
        hook_text = generate_hook_text(person, issue_type, title)
        news_summary = summarize_news(title, summary)

        results.append({
            "run_id": today,
            "category": category,        # âœ… ì¹´í…Œê³ ë¦¬ ì¶”ê°€
            "person": person,            # âœ… celebrity â†’ person
            "issue_type": issue_type,
            "news_title": title,
            "news_url": link,
            "news_summary": news_summary,
            "silhouette_desc": silhouette_desc,
            "hook_text": hook_text,
            "ìƒíƒœ": "ì¤€ë¹„",  # ì‚¬ìš©ìê°€ "ëŒ€ê¸°"ë¡œ ë³€ê²½í•´ì•¼ ì²˜ë¦¬ë¨
        })

    print(f"[SHORTS] '{person}' ê´€ë ¨ {len(results)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
    return results


def collect_and_score_news(
    max_per_feed: int = 10,
    total_limit: int = 20,
    categories: List[str] = None,
    score_top_n: int = 5,
    min_score: float = 30,
) -> List[Dict[str, Any]]:
    """
    ë‰´ìŠ¤ ìˆ˜ì§‘ + ë°”ì´ëŸ´ ì ìˆ˜í™” í†µí•© í•¨ìˆ˜

    1. RSSì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
    2. ê° ë‰´ìŠ¤ì˜ ë°”ì´ëŸ´ ì ì¬ë ¥ ë¶„ì„ (ëŒ“ê¸€ ìˆ˜, ë…¼ìŸì„±)
    3. ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬

    Args:
        max_per_feed: í”¼ë“œë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ìˆ˜
        total_limit: ì „ì²´ ìµœëŒ€ ìˆ˜ì§‘ ìˆ˜
        categories: ìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ ëª©ë¡
        score_top_n: ì ìˆ˜í™”í•  ìƒìœ„ Nê°œ (API í˜¸ì¶œ ìµœì†Œí™”)
        min_score: ìµœì†Œ ë°”ì´ëŸ´ ì ìˆ˜

    Returns:
        ì ìˆ˜ìˆœ ì •ë ¬ëœ ë‰´ìŠ¤ ëª©ë¡ (viral_score, script_hints í¬í•¨)
    """
    print("[SHORTS] === ë‰´ìŠ¤ ìˆ˜ì§‘ + ë°”ì´ëŸ´ ì ìˆ˜í™” ì‹œì‘ ===")

    # 1) RSSì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
    news_items = collect_entertainment_news(
        max_per_feed=max_per_feed,
        total_limit=total_limit,
        categories=categories,
    )

    if not news_items:
        print("[SHORTS] ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
        return []

    print(f"[SHORTS] {len(news_items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ, ìƒìœ„ {score_top_n}ê°œ ì ìˆ˜í™” ì¤‘...")

    # 2) ìƒìœ„ Nê°œë§Œ ì ìˆ˜í™” (API í˜¸ì¶œ ë¹„ìš© ì ˆê°)
    top_items = news_items[:score_top_n]
    scored_items = []

    for item in top_items:
        url = item.get("news_url", "")
        issue_type = item.get("issue_type", "ê·¼í™©")

        # ë°”ì´ëŸ´ ì ì¬ë ¥ ë¶„ì„ (ëŒ“ê¸€ ìˆ˜ì§‘ + ì ìˆ˜í™”)
        analysis = analyze_news_viral_potential(url, issue_type)

        # ê²°ê³¼ ë³‘í•©
        item_with_score = item.copy()
        item_with_score["viral_score"] = analysis["viral_score"]
        item_with_score["script_hints"] = analysis["script_hints"]
        item_with_score["comments_summary"] = {
            "count": analysis["comments_data"].get("comment_count", 0),
            "top_keywords": analysis["comments_data"].get("top_keywords", []),
            "pro_ratio": analysis["comments_data"].get("pro_ratio", 0.5),
        }

        # ìµœì†Œ ì ìˆ˜ ì´ìƒë§Œ í¬í•¨
        if analysis["viral_score"]["total_score"] >= min_score:
            scored_items.append(item_with_score)
            print(f"  âœ… {item['person']}: ì ìˆ˜={analysis['viral_score']['total_score']}, ë“±ê¸‰={analysis['viral_score']['grade']}")
        else:
            print(f"  âŒ {item['person']}: ì ìˆ˜={analysis['viral_score']['total_score']} (ìµœì†Œ {min_score} ë¯¸ë‹¬)")

    # 3) ì ìˆ˜ìˆœ ì •ë ¬
    scored_items.sort(key=lambda x: x["viral_score"]["total_score"], reverse=True)

    print(f"[SHORTS] ë°”ì´ëŸ´ ì ìˆ˜í™” ì™„ë£Œ: {len(scored_items)}ê°œ ë‰´ìŠ¤ (ì ìˆ˜ {min_score}+ ê¸°ì¤€)")
    return scored_items


def get_best_news_for_shorts(
    categories: List[str] = None,
    min_score: float = 40,
) -> Optional[Dict[str, Any]]:
    """
    ì‡¼ì¸  ì œì‘ì— ê°€ì¥ ì í•©í•œ ë‰´ìŠ¤ 1ê°œ ë°˜í™˜

    Args:
        categories: ìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬
        min_score: ìµœì†Œ ë°”ì´ëŸ´ ì ìˆ˜

    Returns:
        ê°€ì¥ ì ìˆ˜ ë†’ì€ ë‰´ìŠ¤ (ì—†ìœ¼ë©´ None)
    """
    scored_news = collect_and_score_news(
        max_per_feed=10,
        total_limit=15,
        categories=categories,
        score_top_n=5,
        min_score=min_score,
    )

    if not scored_news:
        print("[SHORTS] ë°”ì´ëŸ´ ì ìˆ˜ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
        return None

    best = scored_news[0]
    print(f"[SHORTS] ğŸ”¥ ìµœì  ë‰´ìŠ¤ ì„ ì •: {best['person']} ({best['viral_score']['grade']}ë“±ê¸‰, {best['viral_score']['total_score']}ì )")
    return best
