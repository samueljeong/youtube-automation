"""
ì‡¼ì¸  íŒŒì´í”„ë¼ì¸ - YouTube íŠ¸ë Œë”© ê²€ìƒ‰

YouTube Data APIë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¸ë Œë”© ì‡¼ì¸  ê²€ìƒ‰ ë° ë¶„ì„
+ Google News ì—°ë™ìœ¼ë¡œ ì›ë³¸ ìë£Œ í™•ë³´
"""

import os
import re
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, Optional
from urllib.parse import quote_plus

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# feedparser (ë‰´ìŠ¤ ê²€ìƒ‰ìš©)
try:
    import feedparser
    FEEDPARSER_AVAILABLE = True
except ImportError:
    feedparser = None
    FEEDPARSER_AVAILABLE = False

try:
    from dateutil import parser as dtparser
except ImportError:
    dtparser = None


# ========== Google News ê²€ìƒ‰ ==========

def search_google_news(
    query: str,
    max_results: int = 5,
    hours_ago: int = 72,
) -> List[Dict[str, Any]]:
    """
    Google News RSSë¡œ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰

    Args:
        query: ê²€ìƒ‰ì–´ (ì˜ˆ: "ë°•ë‚˜ë˜ ë…¼ë€")
        max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
        hours_ago: ìµœê·¼ ëª‡ ì‹œê°„ ì´ë‚´ ê¸°ì‚¬ë§Œ

    Returns:
        [
            {
                "title": "ê¸°ì‚¬ ì œëª©",
                "link": "https://...",
                "summary": "ê¸°ì‚¬ ìš”ì•½",
                "published_at": "2025-12-28T...",
                "source": "ì—°í•©ë‰´ìŠ¤",
            },
            ...
        ]
    """
    if not FEEDPARSER_AVAILABLE:
        print("[NEWS] feedparser ëª¨ë“ˆ ì—†ìŒ - ë‰´ìŠ¤ ê²€ìƒ‰ ë¶ˆê°€")
        return []

    # Google News RSS URL
    q = quote_plus(query)
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"

    print(f"[NEWS] '{query}' ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")

    try:
        feed = feedparser.parse(url)
        entries = feed.entries[:max_results * 2]  # í•„í„°ë§ ì—¬ìœ ë¶„

        if not entries:
            print(f"[NEWS] '{query}' ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ")
            return []

        # ì‹œê°„ í•„í„°
        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours_ago)
        results = []

        for entry in entries:
            title = getattr(entry, "title", "")
            link = getattr(entry, "link", "")
            summary = getattr(entry, "summary", "")
            published = getattr(entry, "published", None)

            # ë°œí–‰ ì‹œê°„ íŒŒì‹±
            published_at = None
            if published and dtparser:
                try:
                    published_at = dtparser.parse(published).astimezone(timezone.utc)
                except Exception:
                    pass

            # ì‹œê°„ í•„í„° ì ìš©
            if published_at and published_at < cutoff_time:
                continue

            # ì¶œì²˜ ì¶”ì¶œ (ì œëª©ì—ì„œ " - ì¶œì²˜ëª…" í˜•íƒœ)
            source = ""
            if " - " in title:
                parts = title.rsplit(" - ", 1)
                if len(parts) == 2:
                    title = parts[0].strip()
                    source = parts[1].strip()

            results.append({
                "title": title,
                "link": link,
                "summary": summary.replace("<b>", "").replace("</b>", ""),  # HTML íƒœê·¸ ì œê±°
                "published_at": published_at.isoformat() if published_at else "",
                "source": source,
            })

            if len(results) >= max_results:
                break

        print(f"[NEWS] '{query}': {len(results)}ê°œ ê¸°ì‚¬ ë°œê²¬")
        return results

    except Exception as e:
        print(f"[NEWS] ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


def enrich_topic_with_news(topic: Dict[str, Any]) -> Dict[str, Any]:
    """
    YouTube íŠ¸ë Œë”© ì£¼ì œì— ë‰´ìŠ¤ ê¸°ì‚¬ ì¶”ê°€

    ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜ (ì €ì¥ ìŠ¤í‚µìš©)
    """
    person = topic.get("topic", "")
    issue = topic.get("issue", "")

    if not person:
        return None

    # ê²€ìƒ‰ì–´ êµ¬ì„± (ì¸ë¬¼ + ì´ìŠˆ)
    if issue and issue not in ["ì†Œì‹", "íŠ¸ë Œë”©"]:
        search_query = f"{person} {issue}"
    else:
        search_query = person

    # ë‰´ìŠ¤ ê²€ìƒ‰
    news_articles = search_google_news(search_query, max_results=3, hours_ago=72)

    if not news_articles:
        # ì¸ë¬¼ ì´ë¦„ë§Œìœ¼ë¡œ ì¬ê²€ìƒ‰
        news_articles = search_google_news(person, max_results=3, hours_ago=72)

    if not news_articles:
        print(f"[NEWS] âŒ '{person}' ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ - ìŠ¤í‚µ")
        return None

    # ë‰´ìŠ¤ ì •ë³´ ì¶”ê°€
    topic["news_articles"] = news_articles
    topic["primary_news"] = news_articles[0]  # ëŒ€í‘œ ê¸°ì‚¬

    print(f"[NEWS] âœ… '{person}': {len(news_articles)}ê°œ ë‰´ìŠ¤ ê¸°ì‚¬ ì—°ë™")
    return topic


# ========== YouTube API ==========

def get_youtube_client():
    """YouTube Data API í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    api_key = os.environ.get("YOUTUBE_API_KEY")
    if not api_key:
        raise ValueError("YOUTUBE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    return build("youtube", "v3", developerKey=api_key)


# ì—°ì˜ˆì¸/ìœ ëª…ì¸ ì´ë¦„ DB (ìì£¼ ë“±ì¥í•˜ëŠ” ì¸ë¬¼)
CELEBRITY_NAMES = {
    # ì•„ì´ëŒ
    "ë‰´ì§„ìŠ¤", "ì•„ì´ë¸Œ", "ì—ìŠ¤íŒŒ", "ë¥´ì„¸ë¼í•Œ", "ì¹´ë¦¬ë‚˜", "ìœˆí„°", "ì§€ì ¤", "ë‹ë‹",
    "ë¯¼ì§€", "í•˜ë‹ˆ", "ë‹¤ë‹ˆì—˜", "í•´ë¦°", "í˜œì¸", "ì¥ì›ì˜", "ì•ˆìœ ì§„", "ë ˆì´",
    "ì‚¬ì¿ ë¼", "ì¹´ì¦ˆí•˜", "í™ì€ì±„", "ê¹€ì±„ì›", "í—ˆìœ¤ì§„", "BTS", "ë°©íƒ„ì†Œë…„ë‹¨",
    "ì§€ë¯¼", "ì •êµ­", "ë·”", "ìŠˆê°€", "RM", "ì§„", "ì œì´í™‰", "ë¸”ë™í•‘í¬", "ì œë‹ˆ",
    "ì§€ìˆ˜", "ë¡œì œ", "ë¦¬ì‚¬", "íŠ¸ì™€ì´ìŠ¤", "ë‚˜ì—°", "ì •ì—°", "ëª¨ëª¨", "ì‚¬ë‚˜", "ì§€íš¨",
    "ë¯¸ë‚˜", "ë‹¤í˜„", "ì±„ì˜", "ì¯”ìœ„", "ìŠ¤íŠ¸ë ˆì´í‚¤ì¦ˆ", "ë°©ì°¬", "ë¦¬ë…¸", "ì°½ë¹ˆ",
    "í˜„ì§„", "í•œ", "í•„ë¦­ìŠ¤", "ìŠ¹ë¯¼", "ì•„ì´ì—”", "ì„¸ë¸í‹´", "ì—ìŠ¤ì¿±ìŠ¤", "ì •í•œ",
    "ì¡°ìŠˆì•„", "ì¤€", "í˜¸ì‹œ", "ì›ìš°", "ìš°ì§€", "ë””ì—ì‡", "ë¯¼ê·œ", "ë„ê²¸", "ìŠ¹ê´€",
    "ë²„ë…¼", "ë””ë…¸", "ì—”ì‹œí‹°", "íƒœìš©", "ë„ì˜", "ì¬í˜„", "ë§ˆí¬", "í•´ì°¬", "ìŸˆë‹ˆ",
    "ìœ íƒ€", "í…", "ë£¨ì¹´ìŠ¤", "ì²œëŸ¬", "ì§€ì„±", "ëŸ°ì¥”", "ì œë…¸", "ìƒ¤ì˜¤ì¥”", "í—¨ë“œë¦¬",

    # ë°°ìš°
    "ì†¡í˜œêµ", "ì „ì§€í˜„", "ê¹€íƒœí¬", "ì†ì˜ˆì§„", "ê³µìœ ", "í˜„ë¹ˆ", "ì´ë¯¼í˜¸", "ê¹€ìˆ˜í˜„",
    "ë°•ì„œì¤€", "ì†¡ê°•", "ì°¨ì€ìš°", "ì´ë„í˜„", "ë³€ìš°ì„", "ê¹€ì§€ì›", "ì‹ ì„¸ê²½", "ë°•ë¯¼ì˜",
    "í•œì†Œí¬", "ê¹€ìœ ì •", "ì•„ì´ìœ ", "ìˆ˜ì§€", "ë°•ë³´ì˜", "ê¹€ê³ ì€", "ì „ì†Œë¯¼", "ì´ê´‘ìˆ˜",
    "ìœ ì¬ì„", "ê°•í˜¸ë™", "ì‹ ë™ì—½", "ì´íš¨ë¦¬", "ë¹„", "ì •ìš°ì„±", "ì´ì •ì¬", "í•˜ì •ìš°",
    "ë§ˆë™ì„", "í™©ì •ë¯¼", "ì¡°ì¸ì„±", "ì†¡ì¤‘ê¸°", "ë°•ë³´ê²€", "ì´ì¢…ì„", "ë‚¨ì£¼í˜", "ê³µíš¨ì§„",

    # MC/ê°œê·¸ë§¨
    "ë°•ë‚˜ë˜", "ì´ì˜ì§€", "ì „í˜„ë¬´", "ê¹€ì¢…êµ­", "í•˜í•˜", "ì§€ì„ì§„", "ì–‘ì„¸ì°¬", "ì†¡ì§€íš¨",
    "ë‚˜ì˜ì„", "ì¡°ì„¸í˜¸", "ë‚¨ì°½í¬", "ê¹€ì‹ ì˜", "ì•ˆì˜ë¯¸", "ì¥ë„ì—°", "ì´ì€ì§€", "ë¬¸ì„¸ìœ¤",

    # ìš´ë™ì„ ìˆ˜
    "ì†í¥ë¯¼", "ê¹€ë¯¼ì¬", "ì´ê°•ì¸", "í™©í¬ì°¬", "ì˜¤íƒ€ë‹ˆ", "ë¥˜í˜„ì§„", "ê¹€í•˜ì„±", "ì´ì •í›„",
    "ë°•ì„¸ë¦¬", "ê¹€ì—°ì•„", "ì„ì˜ì›…", "ì˜íƒ", "ì´ì°¬ì›", "ì •ë™ì›",

    # ìœ íŠœë²„/ì¸í”Œë£¨ì–¸ì„œ
    "ì¹¨ì°©ë§¨", "ì£¼í˜¸ë¯¼", "í’ì", "ì¯”ì–‘", "ë¨¹ë°©", "ë»‘ê°€", "ë³´ê²¸", "ëŒ€ë„ì„œê´€",
}

# ì œì™¸í•  ì¼ë°˜ ë‹¨ì–´
COMMON_WORDS_BLACKLIST = {
    # ì¼ë°˜ ë‹¨ì–´
    "ì œë°œ", "ì •ë§", "ì§„ì§œ", "í•˜ì§€ë§Œ", "ê·¸ë˜ì„œ", "ë•Œë¬¸", "ì˜¤ëŠ˜", "ë‚´ì¼", "ì–´ì œ",
    "ì˜ˆì „", "ì²˜ëŸ¼", "ê°™ì´", "í•¨ê»˜", "ìš°ë¦¬", "ë‚˜ëŠ”", "ë„ˆëŠ”", "ì´ê²ƒ", "ì €ê²ƒ",
    "ì—¬ê¸°", "ì €ê¸°", "ì§€ê¸ˆ", "ë‚˜ì¤‘", "ë¨¼ì €", "ë‹¤ìŒ", "ë§ˆì§€ë§‰", "ì²˜ìŒ", "ê²°êµ­",
    "ì•„ì§", "ë²Œì¨", "ì´ë¯¸", "ê³„ì†", "ë‹¤ì‹œ", "ë˜", "ë”", "ëœ", "ë§¤ìš°", "ë„ˆë¬´",
    "ì™„ì „", "ì§„ì‹¬", "ì‚¬ì‹¤", "ê·¼ë°", "ê·¸ëƒ¥", "ì¼ë‹¨", "í˜¹ì‹œ", "ì•„ë§ˆ", "ë‹¹ì—°",
    # ì‡¼ì¸  ê´€ë ¨
    "ì‡¼ì¸ ", "shorts", "ì˜ìƒ", "ë‰´ìŠ¤", "ì†ë³´", "ê¸´ê¸‰", "ë‹¨ë…", "ìµœì´ˆ", "ê³µê°œ",
    "ë°˜ì‘", "ë¦¬ì•¡ì…˜", "ìš”ì•½", "ì •ë¦¬", "ëª¨ìŒ", "í•˜ì´ë¼ì´íŠ¸", "ì˜ˆê³ ", "í‹°ì €",
    # ê°ì • í‘œí˜„
    "ì¶©ê²©", "ê°ë™", "ì›ƒìŒ", "ëˆˆë¬¼", "ì†Œë¦„", "ëŒ€ë°•", "ì‹¤í™”", "ë ˆì „ë“œ", "ë¯¸ì³¤",
}


def search_trending_shorts(
    query: str = "ì—°ì˜ˆ ë‰´ìŠ¤",
    max_results: int = 20,
    hours_ago: int = 24,
    order: str = "viewCount",
    region_code: str = "KR",
) -> List[Dict[str, Any]]:
    """
    YouTubeì—ì„œ íŠ¸ë Œë”© ì‡¼ì¸  ê²€ìƒ‰

    Args:
        query: ê²€ìƒ‰ì–´
        max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
        hours_ago: ëª‡ ì‹œê°„ ì´ë‚´ ì˜ìƒ
        order: ì •ë ¬ ê¸°ì¤€ (viewCount, date, rating, relevance)
        region_code: ì§€ì—­ ì½”ë“œ

    Returns:
        [
            {
                "video_id": "...",
                "title": "...",
                "channel_title": "...",
                "published_at": "...",
                "view_count": 123456,
                "like_count": 1234,
                "comment_count": 56,
                "duration_seconds": 45,
                "thumbnail_url": "...",
            },
            ...
        ]
    """
    try:
        youtube = get_youtube_client()

        # ê²€ìƒ‰ ì‹œê°„ ë²”ìœ„ ì„¤ì •
        published_after = (datetime.now(timezone.utc) - timedelta(hours=hours_ago)).isoformat()

        print(f"[YouTube] ê²€ìƒ‰ ì¤‘: '{query}' (ìµœê·¼ {hours_ago}ì‹œê°„, {order}ìˆœ)")

        # 1ë‹¨ê³„: ê²€ìƒ‰
        search_response = youtube.search().list(
            q=query,
            part="snippet",
            type="video",
            videoDuration="short",  # Shorts (60ì´ˆ ì´í•˜)
            order=order,
            publishedAfter=published_after,
            regionCode=region_code,
            maxResults=max_results,
            relevanceLanguage="ko",
        ).execute()

        video_ids = [item["id"]["videoId"] for item in search_response.get("items", [])]

        if not video_ids:
            print("[YouTube] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return []

        # 2ë‹¨ê³„: ë¹„ë””ì˜¤ ìƒì„¸ ì •ë³´ ì¡°íšŒ
        videos_response = youtube.videos().list(
            part="snippet,statistics,contentDetails",
            id=",".join(video_ids),
        ).execute()

        results = []
        for item in videos_response.get("items", []):
            video_id = item["id"]
            snippet = item.get("snippet", {})
            statistics = item.get("statistics", {})
            content_details = item.get("contentDetails", {})

            # ì˜ìƒ ê¸¸ì´ íŒŒì‹± (PT45S â†’ 45ì´ˆ)
            duration_str = content_details.get("duration", "PT0S")
            duration_seconds = parse_duration(duration_str)

            # ShortsëŠ” 60ì´ˆ ì´í•˜ë§Œ
            if duration_seconds > 60:
                continue

            results.append({
                "video_id": video_id,
                "title": snippet.get("title", ""),
                "channel_title": snippet.get("channelTitle", ""),
                "channel_id": snippet.get("channelId", ""),
                "published_at": snippet.get("publishedAt", ""),
                "description": snippet.get("description", ""),
                "view_count": int(statistics.get("viewCount", 0)),
                "like_count": int(statistics.get("likeCount", 0)),
                "comment_count": int(statistics.get("commentCount", 0)),
                "duration_seconds": duration_seconds,
                "thumbnail_url": snippet.get("thumbnails", {}).get("high", {}).get("url", ""),
            })

        # ì¡°íšŒìˆ˜ìˆœ ì •ë ¬
        results.sort(key=lambda x: x["view_count"], reverse=True)

        print(f"[YouTube] {len(results)}ê°œ ì‡¼ì¸  ë°œê²¬")
        return results

    except HttpError as e:
        print(f"[YouTube] API ì˜¤ë¥˜: {e}")
        return []
    except Exception as e:
        print(f"[YouTube] ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


def parse_duration(duration_str: str) -> int:
    """
    ISO 8601 durationì„ ì´ˆë¡œ ë³€í™˜
    PT45S â†’ 45, PT1M30S â†’ 90
    """
    match = re.match(r'PT(?:(\d+)M)?(?:(\d+)S)?', duration_str)
    if match:
        minutes = int(match.group(1) or 0)
        seconds = int(match.group(2) or 0)
        return minutes * 60 + seconds
    return 0


def calculate_engagement_score(video: Dict[str, Any]) -> float:
    """
    ì˜ìƒ ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚°

    - ì¡°íšŒìˆ˜ (40%): 0~100ë§Œ â†’ 0~100ì 
    - ì¢‹ì•„ìš”ìœ¨ (30%): ì¢‹ì•„ìš”/ì¡°íšŒìˆ˜ ë¹„ìœ¨
    - ëŒ“ê¸€ìˆ˜ (20%): 0~1000 â†’ 0~100ì 
    - ì‹ ì„ ë„ (10%): ìµœê·¼ì¼ìˆ˜ë¡ ë†’ìŒ
    """
    views = video.get("view_count", 0)
    likes = video.get("like_count", 0)
    comments = video.get("comment_count", 0)
    published_at = video.get("published_at", "")

    # ì¡°íšŒìˆ˜ ì ìˆ˜ (0~100)
    view_score = min(views / 10000, 100)  # 100ë§Œ = 100ì 

    # ì¢‹ì•„ìš”ìœ¨ ì ìˆ˜ (0~100)
    like_ratio = (likes / views * 100) if views > 0 else 0
    like_score = min(like_ratio * 10, 100)  # 10% = 100ì 

    # ëŒ“ê¸€ìˆ˜ ì ìˆ˜ (0~100)
    comment_score = min(comments / 10, 100)  # 1000ê°œ = 100ì 

    # ì‹ ì„ ë„ ì ìˆ˜ (0~100)
    recency_score = 100
    if published_at:
        try:
            pub_time = datetime.fromisoformat(published_at.replace("Z", "+00:00"))
            hours_old = (datetime.now(timezone.utc) - pub_time).total_seconds() / 3600
            recency_score = max(0, 100 - hours_old * 4)  # 24ì‹œê°„ í›„ 0ì 
        except:
            pass

    # ê°€ì¤‘ í‰ê· 
    total = (
        view_score * 0.4 +
        like_score * 0.3 +
        comment_score * 0.2 +
        recency_score * 0.1
    )

    return round(total, 1)


def get_video_comments(
    video_id: str,
    max_results: int = 50,
) -> List[Dict[str, Any]]:
    """
    ì˜ìƒ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°

    Returns:
        [{"text": "...", "likes": 10, "author": "..."}, ...]
    """
    try:
        youtube = get_youtube_client()

        response = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            order="relevance",  # ì¸ê¸° ëŒ“ê¸€ ìš°ì„ 
            maxResults=max_results,
            textFormat="plainText",
        ).execute()

        comments = []
        for item in response.get("items", []):
            snippet = item.get("snippet", {}).get("topLevelComment", {}).get("snippet", {})
            comments.append({
                "text": snippet.get("textDisplay", ""),
                "likes": snippet.get("likeCount", 0),
                "author": snippet.get("authorDisplayName", ""),
                "published_at": snippet.get("publishedAt", ""),
            })

        return comments

    except HttpError as e:
        # ëŒ“ê¸€ ë¹„í™œì„±í™”ëœ ì˜ìƒ
        if "commentsDisabled" in str(e):
            print(f"[YouTube] ëŒ“ê¸€ ë¹„í™œì„±í™”: {video_id}")
        else:
            print(f"[YouTube] ëŒ“ê¸€ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []
    except Exception as e:
        print(f"[YouTube] ëŒ“ê¸€ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return []


def extract_celebrity_from_title(title: str) -> Optional[str]:
    """
    ì œëª©ì—ì„œ ì—°ì˜ˆì¸/ìœ ëª…ì¸ ì´ë¦„ ì¶”ì¶œ

    1. CELEBRITY_NAMES DBì—ì„œ ë§¤ì¹­
    2. ì—†ìœ¼ë©´ í•œê¸€ ì´ë¦„ íŒ¨í„´ + ë¸”ë™ë¦¬ìŠ¤íŠ¸ í•„í„°
    """
    # 1. DBì—ì„œ ì§ì ‘ ë§¤ì¹­ (ê°€ì¥ ì •í™•)
    for name in CELEBRITY_NAMES:
        if name in title:
            return name

    # 2. í•œê¸€ ì´ë¦„ íŒ¨í„´ (3ê¸€ì ì„±+ì´ë¦„ í˜•íƒœ)
    # "ê¹€OO", "ì´OO", "ë°•OO" ë“± ì„±ì”¨ë¡œ ì‹œì‘í•˜ëŠ” 3ê¸€ì
    korean_surnames = "ê¹€ì´ë°•ìµœì •ê°•ì¡°ìœ¤ì¥ì„í•œì˜¤ì„œì‹ ê¶Œí™©ì•ˆì†¡ë¥˜í™"
    name_pattern = rf'([{korean_surnames}][ê°€-í£]{{2}})(?:ê°€|ì´|ëŠ”|ì˜|ë¥¼|ì—ê²Œ|ì¸¡|ì”¨|,|\s|$)'

    matches = re.findall(name_pattern, title)
    for name in matches:
        # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì²´í¬
        if name not in COMMON_WORDS_BLACKLIST and len(name) >= 2:
            return name

    return None


def extract_trending_topics(videos: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    íŠ¸ë Œë”© ì‡¼ì¸ ì—ì„œ ì£¼ì œ ì¶”ì¶œ (ì—°ì˜ˆì¸ ì¤‘ì‹¬)

    Returns:
        [
            {
                "topic": "ë°•ë‚˜ë˜",
                "video_count": 5,
                "total_views": 1234567,
                "sample_videos": [...],
                "issue": "ë…¼ë€",
            },
            ...
        ]
    """
    from collections import defaultdict

    # ì—°ì˜ˆì¸ë³„ ì˜ìƒ ê·¸ë£¹í™”
    celebrity_videos = defaultdict(list)

    # ì´ìŠˆ í‚¤ì›Œë“œ
    issue_keywords = [
        "ë…¼ë€", "ê°‘ì§ˆ", "í­ë¡œ", "ê³ ë°±", "ê²°í˜¼", "ì´í˜¼", "ì—´ì• ", "íŒŒí˜¼",
        "ì»´ë°±", "ì‚¬ê³¼", "í•´ëª…", "ê·¼í™©", "ë³µê·€", "ì€í‡´", "íƒˆí‡´", "ì†Œì‹",
    ]

    for video in videos:
        title = video.get("title", "")

        # ì—°ì˜ˆì¸ ì´ë¦„ ì¶”ì¶œ
        celebrity = extract_celebrity_from_title(title)

        if celebrity:
            # ì´ìŠˆ í‚¤ì›Œë“œ ì¶”ì¶œ
            found_issue = None
            for kw in issue_keywords:
                if kw in title:
                    found_issue = kw
                    break

            video["detected_celebrity"] = celebrity
            video["detected_issue"] = found_issue or "ì†Œì‹"
            celebrity_videos[celebrity].append(video)

    # ì£¼ì œë³„ ì§‘ê³„
    topics = []
    seen_video_ids = set()  # ì¤‘ë³µ ì˜ìƒ ë°©ì§€

    for celebrity, vids in celebrity_videos.items():
        # ì¤‘ë³µ ì˜ìƒ ì œê±°
        unique_vids = []
        for v in vids:
            vid = v.get("video_id")
            if vid and vid not in seen_video_ids:
                seen_video_ids.add(vid)
                unique_vids.append(v)

        if not unique_vids:
            continue

        # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ì´ìŠˆ ì°¾ê¸°
        issues = [v.get("detected_issue", "ì†Œì‹") for v in unique_vids]
        main_issue = max(set(issues), key=issues.count)

        topics.append({
            "topic": celebrity,
            "issue": main_issue,
            "video_count": len(unique_vids),
            "total_views": sum(v.get("view_count", 0) for v in unique_vids),
            "avg_engagement": sum(calculate_engagement_score(v) for v in unique_vids) / len(unique_vids),
            "sample_videos": unique_vids[:3],
        })

    # ì—°ì˜ˆì¸ì„ ëª» ì°¾ì€ ê²½ìš°: ìƒìœ„ ì˜ìƒì„ ì œëª© ê¸°ì¤€ìœ¼ë¡œ ì¶”ê°€
    if not topics and videos:
        for video in videos[:5]:
            vid = video.get("video_id")
            if vid in seen_video_ids:
                continue
            seen_video_ids.add(vid)

            title = video.get("title", "")
            # ì œëª© ì•ë¶€ë¶„ì„ ì£¼ì œë¡œ ì‚¬ìš© (ìµœëŒ€ 15ì)
            topic_name = title[:15].strip()
            if not topic_name:
                continue

            topics.append({
                "topic": topic_name,
                "issue": "íŠ¸ë Œë”©",
                "video_count": 1,
                "total_views": video.get("view_count", 0),
                "avg_engagement": calculate_engagement_score(video),
                "sample_videos": [video],
            })

    # ì¡°íšŒìˆ˜ìˆœ ì •ë ¬
    topics.sort(key=lambda x: x["total_views"], reverse=True)

    return topics[:10]  # ìƒìœ„ 10ê°œ


def search_shorts_by_category(
    category: str = "ì—°ì˜ˆì¸",
    hours_ago: int = 24,
    max_results: int = 30,
) -> Dict[str, Any]:
    """
    ì¹´í…Œê³ ë¦¬ë³„ íŠ¸ë Œë”© ì‡¼ì¸  ê²€ìƒ‰ + ì£¼ì œ ë¶„ì„

    Args:
        category: ì—°ì˜ˆì¸ / ìš´ë™ì„ ìˆ˜ / êµ­ë½•
        hours_ago: ê²€ìƒ‰ ì‹œê°„ ë²”ìœ„
        max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜

    Returns:
        {
            "videos": [...],
            "topics": [...],
            "best_topic": {...},
        }
    """
    # ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ì–´
    queries = {
        "ì—°ì˜ˆì¸": ["ì—°ì˜ˆ ë‰´ìŠ¤ ì‡¼ì¸ ", "ì•„ì´ëŒ ê·¼í™©", "ì—°ì˜ˆì¸ ë…¼ë€"],
        "ìš´ë™ì„ ìˆ˜": ["ìŠ¤í¬ì¸  ë‰´ìŠ¤ ì‡¼ì¸ ", "ì¶•êµ¬ ì„ ìˆ˜", "ì•¼êµ¬ ì„ ìˆ˜"],
        "êµ­ë½•": ["í•œêµ­ ìë‘", "K-ë¬¸í™”", "ì™¸êµ­ì¸ ë°˜ì‘"],
    }

    category_queries = queries.get(category, queries["ì—°ì˜ˆì¸"])

    all_videos = []
    seen_ids = set()

    for query in category_queries:
        videos = search_trending_shorts(
            query=query,
            max_results=max_results // len(category_queries),
            hours_ago=hours_ago,
        )

        for v in videos:
            if v["video_id"] not in seen_ids:
                v["engagement_score"] = calculate_engagement_score(v)
                all_videos.append(v)
                seen_ids.add(v["video_id"])

    # ì°¸ì—¬ë„ìˆœ ì •ë ¬
    all_videos.sort(key=lambda x: x["engagement_score"], reverse=True)

    # ì£¼ì œ ì¶”ì¶œ
    topics = extract_trending_topics(all_videos)

    return {
        "videos": all_videos,
        "topics": topics,
        "best_topic": topics[0] if topics else None,
        "category": category,
        "search_time": datetime.now(timezone.utc).isoformat(),
    }


def get_best_shorts_topic(
    categories: List[str] = None,
    min_engagement: float = 30,
) -> Optional[Dict[str, Any]]:
    """
    ì‡¼ì¸  ì œì‘ì— ê°€ì¥ ì í•©í•œ íŠ¸ë Œë”© ì£¼ì œ ì°¾ê¸°

    Args:
        categories: ê²€ìƒ‰í•  ì¹´í…Œê³ ë¦¬ (Noneì´ë©´ ì „ì²´)
        min_engagement: ìµœì†Œ ì°¸ì—¬ë„ ì ìˆ˜

    Returns:
        {
            "topic": "ë°•ë‚˜ë˜ ê°‘ì§ˆ",
            "category": "ì—°ì˜ˆì¸",
            "video_count": 5,
            "total_views": 1234567,
            "sample_videos": [...],
            "top_comments": [...],
        }
    """
    if categories is None:
        categories = ["ì—°ì˜ˆì¸"]

    best_topic = None
    best_score = 0

    for category in categories:
        print(f"[YouTube] === {category} ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ ===")

        result = search_shorts_by_category(category=category)

        if result["best_topic"]:
            topic = result["best_topic"]
            score = topic["avg_engagement"]

            print(f"  ğŸ”¥ {topic['topic']}: {topic['video_count']}ê°œ ì˜ìƒ, {topic['total_views']:,}íšŒ ì¡°íšŒ")

            if score > best_score and score >= min_engagement:
                best_score = score
                best_topic = topic
                best_topic["category"] = category

                # ìƒìœ„ ì˜ìƒì˜ ëŒ“ê¸€ ìˆ˜ì§‘
                if topic["sample_videos"]:
                    top_video = topic["sample_videos"][0]
                    comments = get_video_comments(top_video["video_id"], max_results=20)
                    best_topic["top_comments"] = comments[:10]

    if best_topic:
        print(f"[YouTube] âœ… ìµœì  ì£¼ì œ: {best_topic['topic']} (ì°¸ì—¬ë„ {best_score:.1f})")
    else:
        print(f"[YouTube] âŒ ì í•©í•œ ì£¼ì œ ì—†ìŒ (ìµœì†Œ ì°¸ì—¬ë„ {min_engagement} ë¯¸ë‹¬)")

    return best_topic


def youtube_to_news_format(topic: Dict[str, Any]) -> Dict[str, Any]:
    """
    YouTube ì£¼ì œë¥¼ ê¸°ì¡´ ë‰´ìŠ¤ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    (ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ê³¼ í˜¸í™˜)

    topicì— news_articlesê°€ ìˆìœ¼ë©´ ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ë„ í¬í•¨
    """
    from datetime import datetime, timezone

    # ì¸ë¬¼ ì´ë¦„ (topicì´ ì´ì œ ì—°ì˜ˆì¸ ì´ë¦„)
    person = topic.get("topic", "ì—°ì˜ˆì¸")

    # ì´ìŠˆ íƒ€ì… (ìƒˆë¡œìš´ issue í•„ë“œ ì‚¬ìš©)
    issue_type = topic.get("issue", "ê·¼í™©")

    # ìƒ˜í”Œ ì˜ìƒì—ì„œ ì •ë³´ ì¶”ì¶œ
    sample_videos = topic.get("sample_videos", [])
    best_video = sample_videos[0] if sample_videos else {}

    # ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ (ìˆëŠ” ê²½ìš°)
    news_articles = topic.get("news_articles", [])
    primary_news = topic.get("primary_news", {})

    # ë‰´ìŠ¤ ì œëª© = ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª© ìš°ì„ , ì—†ìœ¼ë©´ ì¸ë¬¼ + ì´ìŠˆ
    if primary_news:
        news_title = primary_news.get("title", f"{person} {issue_type}")
        news_url = primary_news.get("link", "")
        news_summary = primary_news.get("summary", "")
        news_source = primary_news.get("source", "")
    else:
        news_title = f"{person} {issue_type}"
        news_url = f"https://youtube.com/watch?v={best_video.get('video_id', '')}" if best_video else ""
        news_summary = f"{topic.get('video_count', 0)}ê°œ ì‡¼ì¸ , {topic.get('total_views', 0):,}íšŒ ì¡°íšŒ"
        news_source = "YouTube"

    return {
        "run_id": datetime.now(timezone.utc).strftime("%Y-%m-%d"),
        "category": topic.get("category", "ì—°ì˜ˆì¸"),
        "person": person,
        "issue_type": issue_type,
        "news_title": news_title,
        "news_url": news_url,
        "news_summary": news_summary,
        "news_source": news_source,
        "viral_score": {
            "total_score": topic.get("avg_engagement", 0),
            "grade": get_grade(topic.get("avg_engagement", 0)),
            "view_score": topic.get("total_views", 0) / 10000,
            "comment_score": 0,
            "controversy_score": 0,
            "recency_score": 100,
        },
        "script_hints": {
            "debate_topic": f"{person} {issue_type} ê´€ë ¨ ë…¼ìŸ",
            "hot_phrases": [c.get("text", "")[:30] for c in topic.get("top_comments", [])[:5]],
            "pro_comments": [],
            "con_comments": [],
        },
        "youtube_source": {
            "topic": person,
            "issue": issue_type,
            "video_count": topic.get("video_count", 0),
            "total_views": topic.get("total_views", 0),
            "sample_videos": sample_videos,
        },
        "news_articles": news_articles,  # ì „ì²´ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡
        "ìƒíƒœ": "ì¤€ë¹„",
    }


def get_grade(score: float) -> str:
    """ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
    if score >= 80:
        return "S"
    elif score >= 60:
        return "A"
    elif score >= 40:
        return "B"
    elif score >= 20:
        return "C"
    return "D"


# CLI í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    import json

    print("=== YouTube íŠ¸ë Œë”© ì‡¼ì¸  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ===\n")

    # ì—°ì˜ˆ ë‰´ìŠ¤ ê²€ìƒ‰
    result = search_shorts_by_category(category="ì—°ì˜ˆì¸", hours_ago=48)

    print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(result['videos'])}ê°œ ì˜ìƒ")
    print(f"ğŸ“ˆ ì¶”ì¶œëœ ì£¼ì œ: {len(result['topics'])}ê°œ")

    if result["topics"]:
        print("\nğŸ”¥ ìƒìœ„ íŠ¸ë Œë”© ì£¼ì œ:")
        for i, topic in enumerate(result["topics"][:5], 1):
            print(f"  {i}. {topic['topic']}: {topic['video_count']}ê°œ ì˜ìƒ, {topic['total_views']:,}íšŒ")

    # ìµœì  ì£¼ì œ ì°¾ê¸°
    print("\n" + "="*50)
    best = get_best_shorts_topic(categories=["ì—°ì˜ˆì¸"])

    if best:
        # ë‰´ìŠ¤ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        news_format = youtube_to_news_format(best)
        print(f"\nğŸ“‹ ë‰´ìŠ¤ í˜•ì‹ ë³€í™˜:")
        print(json.dumps(news_format, ensure_ascii=False, indent=2))
