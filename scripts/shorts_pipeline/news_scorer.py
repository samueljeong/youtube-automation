"""
ì‡¼ì¸  íŒŒì´í”„ë¼ì¸ - ë‰´ìŠ¤ ë°”ì´ëŸ´ ì ìˆ˜í™” ë° ëŒ“ê¸€ ìˆ˜ì§‘

ê¸°ëŠ¥:
1. ë‰´ìŠ¤ ë°”ì´ëŸ´ ì ì¬ë ¥ ì ìˆ˜í™” (ëŒ“ê¸€ ìˆ˜, ë°˜ì‘, ë…¼ìŸì„±)
2. ë„¤ì´ë²„/ë‹¤ìŒ ë‰´ìŠ¤ ëŒ“ê¸€ í¬ë¡¤ë§
3. ì°¬/ë°˜ ì˜ê²¬ ë¶„ë¥˜
4. ëŒ€ë³¸ì— ë°˜ì˜í•  í•µì‹¬ í‘œí˜„ ì¶”ì¶œ
5. Google News ë¦¬ë‹¤ì´ë ‰íŠ¸ URL í•´ê²°
"""

import re
import requests
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs, unquote
import json
import base64

# User-Agent ì„¤ì • (ì°¨ë‹¨ ë°©ì§€)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
}


# ============================================================
# Google News URL ë¦¬ë‹¤ì´ë ‰íŠ¸ í•´ê²°
# ============================================================

def resolve_google_news_url(google_url: str, timeout: int = 10) -> Optional[str]:
    """
    Google News ë¦¬ë‹¤ì´ë ‰íŠ¸ URLì„ ì‹¤ì œ ë‰´ìŠ¤ URLë¡œ ë³€í™˜

    Google News RSSëŠ” ë‹¤ìŒ í˜•íƒœì˜ URLì„ ë°˜í™˜:
    - https://news.google.com/rss/articles/CBMi...
    - https://news.google.com/articles/CBMi...

    ì´ë¥¼ ì‹¤ì œ ë„¤ì´ë²„/ë‹¤ìŒ URLë¡œ ë³€í™˜

    Args:
        google_url: Google News URL
        timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ

    Returns:
        ì‹¤ì œ ë‰´ìŠ¤ URL (ì‹¤íŒ¨ ì‹œ None)
    """
    if not google_url:
        return None

    # ì´ë¯¸ ì‹¤ì œ ë‰´ìŠ¤ URLì¸ ê²½ìš°
    if "naver.com" in google_url or "daum.net" in google_url:
        return google_url

    # Google News URLì´ ì•„ë‹Œ ê²½ìš°
    if "news.google.com" not in google_url:
        return google_url

    try:
        # ë°©ë²• 1: Base64 ë””ì½”ë”© ì‹œë„ (CBMi... íŒ¨í„´)
        decoded_url = _decode_google_news_url(google_url)
        if decoded_url:
            print(f"[NewsScorer] URL ë””ì½”ë”© ì„±ê³µ: {decoded_url[:50]}...")
            return decoded_url

        # ë°©ë²• 2: HTTP ë¦¬ë‹¤ì´ë ‰íŠ¸ ì¶”ì 
        print(f"[NewsScorer] ë¦¬ë‹¤ì´ë ‰íŠ¸ ì¶”ì  ì¤‘: {google_url[:50]}...")
        response = requests.head(
            google_url,
            headers=HEADERS,
            allow_redirects=True,
            timeout=timeout
        )
        final_url = response.url

        # Google URLì—ì„œ ë²—ì–´ë‚¬ëŠ”ì§€ í™•ì¸
        if "news.google.com" not in final_url:
            print(f"[NewsScorer] ë¦¬ë‹¤ì´ë ‰íŠ¸ ì„±ê³µ: {final_url[:50]}...")
            return final_url

        # ë°©ë²• 3: GET ìš”ì²­ìœ¼ë¡œ ì‹¤ì œ í˜ì´ì§€ì—ì„œ ì¶”ì¶œ
        response = requests.get(google_url, headers=HEADERS, timeout=timeout)
        # meta refreshë‚˜ canonical URL ì¶”ì¶œ ì‹œë„
        canonical_match = re.search(r'<link[^>]+rel="canonical"[^>]+href="([^"]+)"', response.text)
        if canonical_match:
            return canonical_match.group(1)

        return google_url  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

    except Exception as e:
        print(f"[NewsScorer] URL í•´ê²° ì‹¤íŒ¨: {e}")
        return google_url


def _decode_google_news_url(url: str) -> Optional[str]:
    """
    Google News URLì—ì„œ Base64 ì¸ì½”ë”©ëœ ì‹¤ì œ URL ì¶”ì¶œ

    URL í˜•ì‹: .../articles/CBMiXXXXX... ë˜ëŠ” .../rss/articles/CBMiXXXXX...
    CBMi... ë¶€ë¶„ì´ Base64 ì¸ì½”ë”©ëœ ì›ë³¸ URL
    """
    try:
        # articles/CBMi... íŒ¨í„´ ì¶”ì¶œ
        match = re.search(r'/articles/([A-Za-z0-9_-]+)', url)
        if not match:
            return None

        encoded = match.group(1)

        # Base64 íŒ¨ë”© ì¶”ê°€
        padding = 4 - len(encoded) % 4
        if padding != 4:
            encoded += "=" * padding

        # URL-safe Base64 â†’ í‘œì¤€ Base64 ë³€í™˜
        encoded = encoded.replace("-", "+").replace("_", "/")

        # ë””ì½”ë”©
        decoded_bytes = base64.b64decode(encoded)

        # URL ì¶”ì¶œ (protobuf í˜•ì‹ì—ì„œ URL ë¬¸ìì—´ ì°¾ê¸°)
        # URLì€ ë³´í†µ http:// ë˜ëŠ” https://ë¡œ ì‹œì‘
        decoded_str = decoded_bytes.decode('utf-8', errors='ignore')
        url_match = re.search(r'https?://[^\s\x00-\x1f"\'<>]+', decoded_str)
        if url_match:
            return url_match.group(0)

        return None

    except Exception as e:
        # ë””ì½”ë”© ì‹¤íŒ¨ëŠ” í”í•¨ - ì¡°ìš©íˆ ì²˜ë¦¬
        return None


# ============================================================
# ë‰´ìŠ¤ ë°”ì´ëŸ´ ì ìˆ˜í™”
# ============================================================

def calculate_viral_score(
    comment_count: int,
    reaction_count: int,
    pro_ratio: float,  # ì°¬ì„± ë¹„ìœ¨ (0~1)
    hours_ago: int,
    issue_type: str = "ê·¼í™©"
) -> Dict[str, Any]:
    """
    ë‰´ìŠ¤ ë°”ì´ëŸ´ ì ì¬ë ¥ ì ìˆ˜ ê³„ì‚°

    ì ìˆ˜ = ëŒ“ê¸€ìˆ˜(40%) + ë°˜ì‘ìˆ˜(30%) + ë…¼ìŸì„±(20%) + ì‹ ì„ ë„(10%)

    Args:
        comment_count: ëŒ“ê¸€ ìˆ˜
        reaction_count: ë°˜ì‘ ìˆ˜ (ì¢‹ì•„ìš” + ì‹«ì–´ìš” ë“±)
        pro_ratio: ì°¬ì„± ë¹„ìœ¨ (0.5ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë…¼ìŸì )
        hours_ago: ë‰´ìŠ¤ ë°œí–‰ í›„ ê²½ê³¼ ì‹œê°„
        issue_type: ì´ìŠˆ ìœ í˜•

    Returns:
        {
            "total_score": 85,
            "comment_score": 35,
            "reaction_score": 25,
            "controversy_score": 18,
            "freshness_score": 7,
            "grade": "A",
            "recommendation": "ê°•ë ¥ ì¶”ì²œ"
        }
    """
    # 1. ëŒ“ê¸€ ì ìˆ˜ (40ì  ë§Œì )
    # 1000ê°œ ì´ìƒ = 40ì , 500ê°œ = 30ì , 100ê°œ = 20ì 
    if comment_count >= 1000:
        comment_score = 40
    elif comment_count >= 500:
        comment_score = 30 + (comment_count - 500) / 50
    elif comment_count >= 100:
        comment_score = 20 + (comment_count - 100) / 40
    elif comment_count >= 50:
        comment_score = 10 + (comment_count - 50) / 5
    else:
        comment_score = comment_count / 5

    # 2. ë°˜ì‘ ì ìˆ˜ (30ì  ë§Œì )
    if reaction_count >= 10000:
        reaction_score = 30
    elif reaction_count >= 5000:
        reaction_score = 25 + (reaction_count - 5000) / 1000
    elif reaction_count >= 1000:
        reaction_score = 15 + (reaction_count - 1000) / 400
    else:
        reaction_score = reaction_count / 100 * 1.5

    # 3. ë…¼ìŸì„± ì ìˆ˜ (20ì  ë§Œì )
    # ì°¬ë°˜ ë¹„ìœ¨ì´ 50:50ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ìŒ
    controversy = 1 - abs(pro_ratio - 0.5) * 2  # 0~1 (0.5ì¼ ë•Œ 1)
    controversy_score = controversy * 20

    # 4. ì‹ ì„ ë„ ì ìˆ˜ (10ì  ë§Œì )
    # 6ì‹œê°„ ì´ë‚´ = 10ì , 24ì‹œê°„ = 7ì , 48ì‹œê°„ = 3ì 
    if hours_ago <= 6:
        freshness_score = 10
    elif hours_ago <= 24:
        freshness_score = 10 - (hours_ago - 6) / 6
    elif hours_ago <= 48:
        freshness_score = 7 - (hours_ago - 24) / 8
    else:
        freshness_score = max(0, 3 - (hours_ago - 48) / 24)

    # 5. ì´ìŠˆ íƒ€ì… ë³´ë„ˆìŠ¤
    issue_bonus = {
        "ë…¼ë€": 10,
        "ì‚¬ê±´": 8,
        "ì—´ì• ": 5,
        "ì»´ë°±": 3,
        "ê·¼í™©": 0,
    }
    bonus = issue_bonus.get(issue_type, 0)

    # ì´ì  ê³„ì‚°
    total_score = min(100, comment_score + reaction_score + controversy_score + freshness_score + bonus)

    # ë“±ê¸‰ íŒì •
    if total_score >= 80:
        grade, recommendation = "S", "ğŸ”¥ ì¦‰ì‹œ ì œì‘"
    elif total_score >= 60:
        grade, recommendation = "A", "âœ… ê°•ë ¥ ì¶”ì²œ"
    elif total_score >= 40:
        grade, recommendation = "B", "ğŸ‘ ì¶”ì²œ"
    elif total_score >= 20:
        grade, recommendation = "C", "âš ï¸ ë³´í†µ"
    else:
        grade, recommendation = "D", "âŒ ë¹„ì¶”ì²œ"

    return {
        "total_score": round(total_score, 1),
        "comment_score": round(comment_score, 1),
        "reaction_score": round(reaction_score, 1),
        "controversy_score": round(controversy_score, 1),
        "freshness_score": round(freshness_score, 1),
        "bonus": bonus,
        "grade": grade,
        "recommendation": recommendation,
    }


# ============================================================
# ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€ í¬ë¡¤ë§
# ============================================================

def extract_naver_article_id(url: str) -> Optional[Tuple[str, str]]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ URLì—ì„œ oid, aid ì¶”ì¶œ

    ì˜ˆ: https://n.news.naver.com/article/001/0014123456
    â†’ oid=001, aid=0014123456
    """
    # íŒ¨í„´ 1: n.news.naver.com/article/OID/AID
    match = re.search(r'n\.news\.naver\.com/article/(\d+)/(\d+)', url)
    if match:
        return match.group(1), match.group(2)

    # íŒ¨í„´ 2: news.naver.com/main/read.naver?...oid=...&aid=...
    parsed = urlparse(url)
    params = parse_qs(parsed.query)
    if 'oid' in params and 'aid' in params:
        return params['oid'][0], params['aid'][0]

    return None


def fetch_naver_comments(
    url: str,
    max_comments: int = 20,
    sort: str = "RECOMMEND"  # RECOMMEND (ê³µê°ìˆœ) or NEW (ìµœì‹ ìˆœ)
) -> Dict[str, Any]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°

    Args:
        url: ë„¤ì´ë²„ ë‰´ìŠ¤ URL
        max_comments: ìµœëŒ€ ëŒ“ê¸€ ìˆ˜
        sort: ì •ë ¬ ë°©ì‹ (RECOMMEND=ê³µê°ìˆœ, NEW=ìµœì‹ ìˆœ)

    Returns:
        {
            "success": True,
            "comment_count": 1234,
            "comments": [
                {
                    "text": "ì´ê±´ ì§„ì§œ ì„  ë„˜ì—ˆë‹¤",
                    "likes": 3200,
                    "dislikes": 150,
                    "sentiment": "negative",  # positive/negative/neutral
                },
                ...
            ],
            "pro_ratio": 0.45,  # ê¸ì • ë¹„ìœ¨
            "top_keywords": ["ê°‘ì§ˆ", "ì„ ë„˜ì—ˆë‹¤", "ì‹¤ë§"],
        }
    """
    article_ids = extract_naver_article_id(url)
    if not article_ids:
        return {"success": False, "error": "ë„¤ì´ë²„ ë‰´ìŠ¤ URLì´ ì•„ë‹™ë‹ˆë‹¤"}

    oid, aid = article_ids

    # ë„¤ì´ë²„ ëŒ“ê¸€ API
    api_url = f"https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json"
    params = {
        "ticket": "news",
        "templateId": "default_society",
        "pool": "cbox5",
        "lang": "ko",
        "country": "KR",
        "objectId": f"news{oid},{aid}",
        "pageSize": max_comments,
        "indexSize": 10,
        "groupId": "",
        "listType": "OBJECT",
        "pageType": "more",
        "page": 1,
        "refresh": "false",
        "sort": sort,
    }

    headers = HEADERS.copy()
    headers["Referer"] = url

    try:
        response = requests.get(api_url, params=params, headers=headers, timeout=10)

        # JSONP ì‘ë‹µ íŒŒì‹±
        text = response.text
        # _callback( ... ) í˜•ì‹ì—ì„œ JSON ì¶”ì¶œ
        match = re.search(r'_callback\((.*)\)', text, re.DOTALL)
        if not match:
            # ìˆœìˆ˜ JSONì¸ ê²½ìš°
            data = response.json()
        else:
            data = json.loads(match.group(1))

        result = data.get("result", {})
        comment_list = result.get("commentList", [])
        total_count = result.get("count", {}).get("total", 0)

        comments = []
        positive_count = 0
        negative_count = 0
        all_text = []

        for c in comment_list:
            text = c.get("contents", "")
            likes = c.get("sympathyCount", 0)
            dislikes = c.get("antipathyCount", 0)

            # ê°ì • ë¶„ì„ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
            sentiment = analyze_sentiment(text)
            if sentiment == "positive":
                positive_count += 1
            elif sentiment == "negative":
                negative_count += 1

            comments.append({
                "text": text,
                "likes": likes,
                "dislikes": dislikes,
                "sentiment": sentiment,
            })
            all_text.append(text)

        # ì°¬ë°˜ ë¹„ìœ¨ ê³„ì‚°
        total_sentiment = positive_count + negative_count
        pro_ratio = positive_count / total_sentiment if total_sentiment > 0 else 0.5

        # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        top_keywords = extract_top_keywords(" ".join(all_text))

        return {
            "success": True,
            "comment_count": total_count,
            "fetched_count": len(comments),
            "comments": comments,
            "pro_ratio": round(pro_ratio, 2),
            "top_keywords": top_keywords,
        }

    except Exception as e:
        print(f"[NewsScorer] ë„¤ì´ë²„ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}


# ============================================================
# ë‹¤ìŒ ë‰´ìŠ¤ ëŒ“ê¸€ í¬ë¡¤ë§
# ============================================================

def fetch_daum_comments(
    url: str,
    max_comments: int = 20
) -> Dict[str, Any]:
    """
    ë‹¤ìŒ ë‰´ìŠ¤ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°

    ë‹¤ìŒ ë‰´ìŠ¤ ëŒ“ê¸€ API í™œìš©
    """
    # ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ ID ì¶”ì¶œ
    match = re.search(r'/v/(\w+)', url)
    if not match:
        return {"success": False, "error": "ë‹¤ìŒ ë‰´ìŠ¤ URLì´ ì•„ë‹™ë‹ˆë‹¤"}

    article_id = match.group(1)

    # ë‹¤ìŒ ëŒ“ê¸€ API
    api_url = f"https://comment.daum.net/apis/v1/ui/single/main/@{article_id}"
    params = {
        "limit": max_comments,
        "sort": "POPULAR",  # POPULAR (ì¸ê¸°ìˆœ) or LATEST (ìµœì‹ ìˆœ)
    }

    try:
        response = requests.get(api_url, params=params, headers=HEADERS, timeout=10)
        data = response.json()

        comment_list = data.get("commentList", [])
        total_count = data.get("totalCount", 0)

        comments = []
        positive_count = 0
        negative_count = 0
        all_text = []

        for c in comment_list:
            text = c.get("content", "")
            likes = c.get("likeCount", 0)
            dislikes = c.get("dislikeCount", 0)

            sentiment = analyze_sentiment(text)
            if sentiment == "positive":
                positive_count += 1
            elif sentiment == "negative":
                negative_count += 1

            comments.append({
                "text": text,
                "likes": likes,
                "dislikes": dislikes,
                "sentiment": sentiment,
            })
            all_text.append(text)

        total_sentiment = positive_count + negative_count
        pro_ratio = positive_count / total_sentiment if total_sentiment > 0 else 0.5
        top_keywords = extract_top_keywords(" ".join(all_text))

        return {
            "success": True,
            "comment_count": total_count,
            "fetched_count": len(comments),
            "comments": comments,
            "pro_ratio": round(pro_ratio, 2),
            "top_keywords": top_keywords,
        }

    except Exception as e:
        print(f"[NewsScorer] ë‹¤ìŒ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}


# ============================================================
# ê°ì • ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
# ============================================================

def analyze_sentiment(text: str) -> str:
    """
    ê°„ë‹¨í•œ ê°ì • ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜)

    Returns: "positive", "negative", "neutral"
    """
    positive_words = [
        "ì¢‹ì•„", "ìµœê³ ", "ì‘ì›", "ëŒ€ë°•", "ë©‹ì§€", "ì˜í–ˆ", "ì¶•í•˜", "ê¸°ëŒ€", "ê°ë™",
        "ì‚¬ë‘", "í–‰ë³µ", "ì›ƒê¸°", "ê·€ì—½", "ì˜ˆì˜", "ì˜ìƒ", "ì¡´ê²½", "í™”ì´íŒ…",
    ]

    negative_words = [
        "ì‹«ì–´", "ìµœì•…", "ì‹¤ë§", "ë³„ë¡œ", "ì“°ë ˆê¸°", "ì“°ë ‰", "ì§„ì§œ", "ì„ ë„˜", "ì„  ë„˜",
        "ê°‘ì§ˆ", "í•™í­", "í­ë¡œ", "ë¹„íŒ", "ë…¼ë€", "í˜ì˜¤", "ì—­ê²¹", "ì§œì¦", "í™”ë‚˜",
        "ì‹¤í™”", "í—", "ì—íœ´", "í•œì‹¬", "ì–´ì´ì—†", "ê·¸ë§Œ", "êº¼ì ¸", "ë‚˜ê°€",
    ]

    text_lower = text.lower()

    pos_count = sum(1 for w in positive_words if w in text_lower)
    neg_count = sum(1 for w in negative_words if w in text_lower)

    if neg_count > pos_count:
        return "negative"
    elif pos_count > neg_count:
        return "positive"
    else:
        return "neutral"


def extract_top_keywords(text: str, top_n: int = 5) -> List[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
    """
    # ë¶ˆìš©ì–´
    stopwords = {
        "ì´", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ë“±", "ë”", "ì¢€", "ì˜", "ì•ˆ", "ëª»",
        "í•˜ë‹¤", "ë˜ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "ê°™ë‹¤", "ë³´ë‹¤", "ë‚˜ë‹¤", "ì£¼ë‹¤",
        "ã…‹ã…‹", "ã…ã…", "ã… ã… ", "ã…œã…œ", "...", "???", "!!!",
    }

    # 2ê¸€ì ì´ìƒ í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ
    words = re.findall(r'[ê°€-í£]{2,}', text)

    # ë¹ˆë„ ê³„ì‚°
    word_count = {}
    for word in words:
        if word not in stopwords and len(word) >= 2:
            word_count[word] = word_count.get(word, 0) + 1

    # ìƒìœ„ Nê°œ
    sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)
    return [w for w, c in sorted_words[:top_n]]


# ============================================================
# ëŒ“ê¸€ ê¸°ë°˜ ëŒ€ë³¸ íŒíŠ¸ ìƒì„±
# ============================================================

def generate_script_hints(comments_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    ìˆ˜ì§‘ëœ ëŒ“ê¸€ì„ ë°”íƒ•ìœ¼ë¡œ ëŒ€ë³¸ ì‘ì„± íŒíŠ¸ ìƒì„±

    Returns:
        {
            "debate_topic": "ê°‘ì§ˆì´ë‹¤ vs ë§¤ë‹ˆì €ê°€ ì˜ˆë¯¼í•˜ë‹¤",
            "pro_arguments": ["ì´ê±´ ì„  ë„˜ì—ˆë‹¤", "ì‚¬ê³¼í•´ì•¼ í•œë‹¤"],
            "con_arguments": ["ë§¤ë‹ˆì €ê°€ ê³¼ë¯¼ë°˜ì‘", "ìš”ì¦˜ ë„ˆë¬´ ì˜ˆë¯¼"],
            "hot_phrases": ["ì„  ë„˜ì—ˆë‹¤", "ì´ê±´ ì¢€...", "ì‹¤ë§ì´ë‹¤"],
            "suggested_scene4": "ëŒ“ê¸€ ë³´ë‹ˆê¹Œ 'ì„  ë„˜ì—ˆë‹¤' vs 'ë§¤ë‹ˆì €ê°€ ì˜ˆë¯¼' ì™„ì „ ê°ˆë ¸ì–´. ë„ˆëŠ” ì–´ëŠ ìª½?",
        }
    """
    if not comments_data.get("success") or not comments_data.get("comments"):
        return {
            "debate_topic": None,
            "pro_arguments": [],
            "con_arguments": [],
            "hot_phrases": [],
            "suggested_scene4": None,
        }

    comments = comments_data["comments"]

    # ì°¬ì„±/ë°˜ëŒ€ ì˜ê²¬ ë¶„ë¥˜
    pro_arguments = []
    con_arguments = []
    hot_phrases = []

    for c in comments:
        text = c["text"]
        sentiment = c["sentiment"]
        likes = c.get("likes", 0)

        # ê³µê° ë†’ì€ ëŒ“ê¸€ì—ì„œ í•µì‹¬ ë¬¸êµ¬ ì¶”ì¶œ
        if likes >= 100:
            # ì§§ì€ ë¬¸êµ¬ ì¶”ì¶œ (15ì ì´í•˜)
            phrases = re.findall(r'[ê°€-í£\s]{5,15}', text)
            for phrase in phrases[:2]:
                phrase = phrase.strip()
                if phrase and phrase not in hot_phrases:
                    hot_phrases.append(phrase)

        # ì°¬ë°˜ ë¶„ë¥˜
        if sentiment == "negative":
            # ë¶€ì •ì  = ë¹„íŒ (ë‹¹ì‚¬ìì—ê²Œ)
            short_text = text[:30] + "..." if len(text) > 30 else text
            if short_text not in pro_arguments:
                pro_arguments.append(short_text)
        elif sentiment == "positive":
            # ê¸ì •ì  = ì˜¹í˜¸
            short_text = text[:30] + "..." if len(text) > 30 else text
            if short_text not in con_arguments:
                con_arguments.append(short_text)

    # ìƒìœ„ 3ê°œë§Œ
    pro_arguments = pro_arguments[:3]
    con_arguments = con_arguments[:3]
    hot_phrases = hot_phrases[:5]

    # ë…¼ìŸ ì£¼ì œ ìƒì„±
    keywords = comments_data.get("top_keywords", [])
    if keywords:
        debate_topic = f"'{keywords[0]}' ë…¼ë€, ì–´ë–»ê²Œ ìƒê°í•´?"
    else:
        debate_topic = None

    # ì”¬4 ì œì•ˆ
    if hot_phrases:
        suggested_scene4 = f"ëŒ“ê¸€ ë³´ë‹ˆê¹Œ '{hot_phrases[0]}' ë§ ë§ë”ë¼. ë„ˆëŠ” ì–´ë–»ê²Œ ìƒê°í•´? ëŒ“ê¸€ë¡œ."
    else:
        suggested_scene4 = None

    return {
        "debate_topic": debate_topic,
        "pro_arguments": pro_arguments,
        "con_arguments": con_arguments,
        "hot_phrases": hot_phrases,
        "suggested_scene4": suggested_scene4,
    }


# ============================================================
# í†µí•© í•¨ìˆ˜: ë‰´ìŠ¤ ë¶„ì„
# ============================================================

def analyze_news_viral_potential(
    url: str,
    issue_type: str = "ê·¼í™©",
    hours_ago: int = 12
) -> Dict[str, Any]:
    """
    ë‰´ìŠ¤ ë°”ì´ëŸ´ ì ì¬ë ¥ ì¢…í•© ë¶„ì„

    Args:
        url: ë‰´ìŠ¤ URL (Google News, ë„¤ì´ë²„, ë‹¤ìŒ ëª¨ë‘ ì§€ì›)
        issue_type: ì´ìŠˆ ìœ í˜•
        hours_ago: ë‰´ìŠ¤ ë°œí–‰ í›„ ê²½ê³¼ ì‹œê°„

    Returns:
        {
            "viral_score": {...},
            "comments_data": {...},
            "script_hints": {...},
            "resolved_url": "ì‹¤ì œ ë‰´ìŠ¤ URL"
        }
    """
    print(f"[NewsScorer] ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘: {url[:50]}...")

    # 1) Google News URLì¸ ê²½ìš° ì‹¤ì œ URLë¡œ í•´ê²°
    resolved_url = url
    if "news.google.com" in url:
        resolved_url = resolve_google_news_url(url) or url
        if resolved_url != url:
            print(f"[NewsScorer] ì‹¤ì œ URL: {resolved_url[:50]}...")

    # 2) ë‰´ìŠ¤ ì†ŒìŠ¤ íŒë³„ ë° ëŒ“ê¸€ ìˆ˜ì§‘
    if "naver.com" in resolved_url or "n.news.naver.com" in resolved_url:
        comments_data = fetch_naver_comments(resolved_url)
    elif "daum.net" in resolved_url or "v.daum.net" in resolved_url:
        comments_data = fetch_daum_comments(resolved_url)
    else:
        # ê¸°íƒ€ ì†ŒìŠ¤ (ì—°í•©ë‰´ìŠ¤, ì¡°ì„ ì¼ë³´ ë“±) - ëŒ“ê¸€ ì—†ìŒ
        comments_data = {"success": False, "error": f"ëŒ“ê¸€ ë¯¸ì§€ì› ì†ŒìŠ¤: {urlparse(resolved_url).netloc}"}

    # ëŒ“ê¸€ ë°ì´í„° ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
    if comments_data.get("success"):
        comment_count = comments_data.get("comment_count", 0)
        pro_ratio = comments_data.get("pro_ratio", 0.5)
        # ë°˜ì‘ ìˆ˜ëŠ” ëŒ“ê¸€ì˜ ì¢‹ì•„ìš” í•©ê³„ë¡œ ì¶”ì •
        reaction_count = sum(c.get("likes", 0) for c in comments_data.get("comments", []))
    else:
        comment_count = 0
        pro_ratio = 0.5
        reaction_count = 0

    # ë°”ì´ëŸ´ ì ìˆ˜ ê³„ì‚°
    viral_score = calculate_viral_score(
        comment_count=comment_count,
        reaction_count=reaction_count,
        pro_ratio=pro_ratio,
        hours_ago=hours_ago,
        issue_type=issue_type,
    )

    # ëŒ€ë³¸ íŒíŠ¸ ìƒì„±
    script_hints = generate_script_hints(comments_data)

    print(f"[NewsScorer] ë¶„ì„ ì™„ë£Œ: ì ìˆ˜={viral_score['total_score']}, ë“±ê¸‰={viral_score['grade']}")

    return {
        "viral_score": viral_score,
        "comments_data": comments_data,
        "script_hints": script_hints,
        "resolved_url": resolved_url,
    }


# ============================================================
# ë‰´ìŠ¤ ëª©ë¡ ì ìˆ˜í™” ë° ì •ë ¬
# ============================================================

def rank_news_by_viral_potential(
    news_items: List[Dict[str, Any]],
    min_score: float = 30,
    top_n: int = 10
) -> List[Dict[str, Any]]:
    """
    ë‰´ìŠ¤ ëª©ë¡ì„ ë°”ì´ëŸ´ ì ì¬ë ¥ ìˆœìœ¼ë¡œ ì •ë ¬

    Args:
        news_items: ë‰´ìŠ¤ ëª©ë¡ (news_collectorì—ì„œ ìˆ˜ì§‘ëœ í˜•ì‹)
        min_score: ìµœì†Œ ì ìˆ˜ (ì´í•˜ëŠ” ì œì™¸)
        top_n: ìƒìœ„ Nê°œë§Œ ë°˜í™˜

    Returns:
        ì ìˆ˜ìˆœ ì •ë ¬ëœ ë‰´ìŠ¤ ëª©ë¡ (viral_score, script_hints ì¶”ê°€ë¨)
    """
    scored_items = []

    for item in news_items:
        url = item.get("news_url", "")
        issue_type = item.get("issue_type", "ê·¼í™©")

        # ë¶„ì„ ìˆ˜í–‰
        analysis = analyze_news_viral_potential(url, issue_type)

        # ê²°ê³¼ ë³‘í•©
        item_with_score = item.copy()
        item_with_score["viral_score"] = analysis["viral_score"]
        item_with_score["script_hints"] = analysis["script_hints"]
        item_with_score["comments_summary"] = {
            "count": analysis["comments_data"].get("comment_count", 0),
            "top_keywords": analysis["comments_data"].get("top_keywords", []),
            "pro_ratio": analysis["comments_data"].get("pro_ratio", 0.5),
        }

        # ìµœì†Œ ì ìˆ˜ ì´ìƒë§Œ í¬í•¨
        if analysis["viral_score"]["total_score"] >= min_score:
            scored_items.append(item_with_score)

    # ì ìˆ˜ìˆœ ì •ë ¬
    scored_items.sort(key=lambda x: x["viral_score"]["total_score"], reverse=True)

    return scored_items[:top_n]
